# å¼ºåŒ–å­¦ä¹ å…¥é—¨

## å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹

### å‘¨åšç£Š

[https://www.bilibili.com/video/BV1LE411G7Xj?from=search&seid=9725909430531578664](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1LE411G7Xj%3Ffrom%3Dsearch%26seid%3D9725909430531578664)

### æå®æ¯…

å°å¤§æå®æ¯…æ•™æˆçš„è¯¾ç¨‹ï¼Œè¿™é—¨è¯¾ä»¥Suttonçš„ä¹¦ç±ä½œä¸ºæ•™æï¼Œå¯¹å¼ºåŒ–å­¦ä¹ é‡Œæ¯”è¾ƒé‡è¦çš„æ¦‚å¿µå’Œç®—æ³•è¿›è¡Œäº†æ•™æˆã€‚

### ç‹æ ‘æ£®è€å¸ˆ

è¯¾ä»¶ï¼š[https://github.com/wangshusen/DeepLearning](https://github.com/wangshusen/DeepLearning)

è®²ä¹‰ï¼š[https://github.com/wangshusen/DRL/blob/master/Notes_CN/chp3.pdf](https://github.com/wangshusen/DRL/blob/master/Notes_CN/chp3.pdf)

### RL China

<https://www.bilibili.com/video/av584041095/>

### ç™¾åº¦

<https://www.bilibili.com/video/BV1yv411i7xd?from=search&seid=9725909430531578664>

## å¼ºåŒ–å­¦ä¹ ä¹¦+ä»£ç 

### Reinforcement Learning: An Introduction

ç¬¬ä¸€æœ¬å¼ºåŒ–å­¦ä¹ çš„ä¹¦ï¼Œä¹Ÿæ˜¯æœ€ç»å…¸çš„ä¹¦ï¼Œã€ŠReinforcement Learning: An Introductionã€‹ Richard S. Sutton and Andrew G. Bartoã€‚ Suttonåœ¨2012å¹´Releaseå‡ºæ¥çš„ï¼Œæ›´æ–°ä¹‹åçš„ç¬¬äºŒç‰ˆã€‚åº”è¯¥ç®—æ˜¯ç›®å‰ä¸ºæ­¢ï¼Œå…³äºå¼ºåŒ–å­¦ä¹ ï¼Œä»‹ç»æœ€ä¸ºè¯¦ç»†ï¼Œå…¨é¢çš„æ•™æä¹‹ä¸€ã€‚David Silverçš„å¼ºåŒ–å­¦ä¹ è§†é¢‘ä¹Ÿæ˜¯æ ¹æ®è¿™æœ¬æ•™æå±•å¼€ï¼Œé…åˆç€çœ‹ï¼Œæ›´å®¹æ˜“ç†è§£ã€‚

- **ä¹¦ç±é…å¥—ä»£ç 1**

<https://github.com/ShangtongZhang/reinforcement-learning-an-introduction>

- **ä¹¦ç±é…å¥—ä»£ç 2**

<https://github.com/dennybritz/reinforcement-learning>

- **é…å¥—è§†é¢‘è¯¾ç¨‹ï¼š**

[DAVID SILVER] (<https://www.davidsilver.uk/>)

### openai  spinningup

OpenAI Spinning Upï¼ˆ äº”æ˜Ÿæ¨èï¼ï¼‰æ·±åº¦å¼ºåŒ–å­¦ä¹ å…¥é—¨çš„æœ€ä½³æ‰‹å†Œï¼Œä»£ç çŸ­å°ç²¾æ‚ï¼Œæ€§èƒ½ä¹Ÿä¸é”™ï¼ŒåŒ…å«äº†åŸºç¡€åŸç†ã€å…³é”®è®ºæ–‡ã€å…³é”®ç®—æ³•è§£é‡Šä»¥åŠå®ç°ã€‚[æ–‡æ¡£](https://spinningup.openai.com/en/latest/)ååˆ†è‰¯å¿ƒï¼Œå’Œä»£ç åŸºæœ¬ä¸€ä¸€å¯¹ç…§ã€‚

- <https://spinningup.openai.com/en/latest/user/introduction.html>

## å¼ºåŒ–å­¦ä¹ å¼€æºå·¥å…·ç®±

ä»£ç åº“ç»è¿‡é‡è›®ç”Ÿé•¿çš„å¹´ä»£åç»ˆäºè¶‹äºç¨³å®šï¼Œåœ¨å­¦ä¹ ã€ç§‘ç ”ã€ç”Ÿäº§çš„ä¸åŒé˜¶æ®µéƒ½æœ‰äº†ååˆ†æˆç†Ÿçš„ä»£ç åº“ï¼Œæˆç†Ÿçš„ä»£ç åº“ä¸ä»…æŒ‡å¥½ç”¨çš„ä»£ç ï¼Œè¿˜éœ€è¦æ¸…æ™°çš„æ–‡æ¡£ï¼Œé…å¥—çš„å·¥å…·ç­‰ã€‚åœ¨æ­¤å„æ¨èä¸€ä¸ªï¼ˆå°‘å³æ˜¯å¤šï¼Œæµ“ç¼©æ‰æ˜¯ç²¾åï¼‰å¦‚ä¸‹ï¼š

### OpenAI Gym

ç›®å‰å¼ºåŒ–å­¦ä¹ ç¼–ç¨‹å®æˆ˜å¸¸ç”¨çš„ç¯å¢ƒå°±æ˜¯OpenAIçš„gymåº“äº†ï¼Œæ”¯æŒPythonè¯­è¨€ç¼–ç¨‹ã€‚OpenAI Gymæ˜¯ä¸€æ¬¾ç”¨äºç ”å‘å’Œæ¯”è¾ƒå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å·¥å…·åŒ…ï¼Œå®ƒæ”¯æŒè®­ç»ƒæ™ºèƒ½ä½“ï¼ˆagentï¼‰åšä»»ä½•äº‹â€”â€”ä»è¡Œèµ°åˆ°ç©Pongæˆ–å›´æ£‹ä¹‹ç±»çš„æ¸¸æˆéƒ½åœ¨èŒƒå›´ä¸­ã€‚

- <https://gym.openai.com/>

### Baselines

https://github.com/openai/baselinesgithub.com/openai/baselines

### stable-baselines3

 [stable-baselines3](https//github.com/DLR-RM/stable-baselines3) ç”± OpenAI çš„ baselines å‘å±•è€Œæ¥ï¼Œå› ä¸º baselines ä¸å¤Ÿç¨³å®šï¼Œäºæ˜¯æœ‰äº† [stable-baselines](https//github.com/hill-a/stable-baselines)ï¼Œæ¥ç€æœ‰äº† v2ï¼Œå†æœ‰äº† PyTorch ç‰ˆçš„ v3ï¼Œç›®å‰ç”± DLR-RM ç»´æŠ¤ã€‚ä¸ä»…[æ–‡æ¡£](https://stable-baselines3.readthedocs.io/)æ¸…æ™°ï¼Œè¿˜æä¾›äº†å¾ˆå¤šå¸¸ç”¨ç¯å¢ƒå’ŒRLç®—æ³•çš„è°ƒä¼˜è¶…å‚æ•°ï¼š[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo).å®ç°äº†å‡ ä¹æ‰€æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚

### Ray/RLlib

[ray/rllib](https://github.com/ray-project/ray)ã€‚UC Berkeley å‡ºå“ï¼Œå·¥ä¸šçº§çš„å¼ºåŒ–å­¦ä¹ åº“ï¼Œä¼˜åŠ¿åœ¨äºåˆ†å¸ƒå¼è®¡ç®—å’Œè‡ªåŠ¨è°ƒå‚ï¼Œæ”¯æŒ TensorFlow/PyTorchï¼Œå¾ˆå¤šå¤§ä¼ä¸šæ¯”å¦‚è°·æ­Œã€äºšé©¬é€Šã€èš‚èšé‡‘æœéƒ½åœ¨ç”¨ã€‚

## PyTorch  + RL  Algorithms

### æ­¥æ­¥æ·±å…¥RL

è¿™ä»½Pytorchå¼ºåŒ–å­¦ä¹ æ•™ç¨‹ä¸€å…±æœ‰å…«ç« ï¼Œä»DQNï¼ˆDeep Q-Learningï¼‰å¼€å§‹ï¼Œæ­¥æ­¥æ·±å…¥ï¼Œæœ€åå‘ä½ å±•ç¤ºRainbowåˆ°åº•æ˜¯ä»€ä¹ˆã€‚

ä¸ä»…æœ‰Jupyter Notebookï¼Œä½œè€…è¿˜åœ¨Colabä¸Šé…ç½®å¥½äº†ä»£ç ï¼Œæ— éœ€å®‰è£…ï¼Œä½ å°±èƒ½ç›´è§‚åœ°æ„Ÿå—åˆ°ç®—æ³•çš„æ•ˆæœï¼Œç”šè‡³è¿˜å¯ä»¥ç›´æ¥åœ¨æ‰‹æœºä¸Šè¿›è¡Œå­¦ä¹ ï¼

Github åœ°å€ï¼š <https://github.com/Curt-Park/rainbow-is-all-you-need>

### CleanRL

CleanRL (Clean Implementation of RL Algorithms)
 tests ci   Code style: black Imports: isort

CleanRL is a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch. The highlight features of CleanRL are:

ğŸ“œ Single-file implementation
Every detail about an algorithm variant is put into a single standalone file.
For example, our ppo_atari.py only has 340 lines of code but contains all implementation details on how PPO works with Atari games, so it is a great reference implementation to read for folks who do not wish to read an entire modular library.
ğŸ“Š Benchmarked Implementation (7+ algorithms and 34+ games at <https://benchmark.cleanrl.dev>)
ğŸ“ˆ Tensorboard Logging
ğŸª› Local Reproducibility via Seeding
ğŸ® Videos of Gameplay Capturing
ğŸ§« Experiment Management with Weights and Biases
ğŸ’¸ Cloud Integration with docker and AWS

Github åœ°å€ï¼š  <https://github.com/vwxyzjn/cleanrl>

### openai - spinningup

openaiçš„spinningupï¼šé‡Œé¢æä¾›äº†ç»å…¸Policy-basedç®—æ³•çš„å¤ç°ï¼Œä¼˜ç‚¹æ˜¯å†™çš„é€šä¿—æ˜“æ‡‚ä¸Šæ‰‹ç®€å•ï¼Œå¹¶ä¸”æ•ˆæœæœ‰ä¿éšœï¼Œè€Œä¸”åŒæ—¶tfå’ŒPytorchçš„æ”¯æŒï¼›ç¼ºç‚¹æ˜¯æ²¡æœ‰value-basedçš„ç®—æ³•ï¼ŒåšDQNç³»åˆ—çš„å°±æ²¡åŠæ³•äº†

### rlpyt

UCBä¸¤ä¸ªå¤§ä½¬å¼€æºçš„rlpytï¼šä¸“é—¨åŸºäºpytorchå®ç°çš„rlæ¡†æ¶ï¼Œæœ‰å•æœº/å¤šæœºåˆ†é…èµ„æºçš„é»‘ç§‘æŠ€ï¼ŒæŒ‚arxivçš„paperé‡Œé¢ä»‹ç»çš„ä¹Ÿæ•ˆæœä¹Ÿä¸é”™ã€‚contributorä»¥å‰ä¹Ÿå†™è¿‡å¦‚ä½•åŠ é€ŸDQNè®­ç»ƒçš„è°ƒå‚æ–¹æ³•

### Deep Reinforcement Learning Algorithms with PyTorch

This repository contains PyTorch implementations of deep reinforcement learning algorithms and environments.

(To help you remember things you learn about machine learning in general write them inÂ [Save All](https://saveall.ai/shared/deck/140&4&3K3uXPazkg4&github_links)Â and try out the public deck there about Fast AI's machine learning textbook.)

Github åœ°å€ï¼š [p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorchâ€‹](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch.git)

å·²å®ç°çš„ç®—æ³•åŒ…æ‹¬ï¼š

- Deep Q Learning (**DQN**) (Mnih et al. 2013)
- **DQN with Fixed Q Targets** (Mnih et al. 2013)
- Double DQN (**DDQN**) (Hado van Hasselt et al. 2015)
- **DDQN with Prioritised Experience Replay** (Schaul et al. 2016)
- **Dueling DDQN** (Wang et al. 2016)
- **REINFORCE** (Williams et al. 1992)
- Deep Deterministic Policy Gradients (**DDPG**) (Lillicrap et al. 2016 )
- Twin Delayed Deep Deterministic Policy Gradients (**TD3**) (Fujimoto et al. 2018)
- Soft Actor-Critic (**SAC & SAC-Discrete**) (Haarnoja et al. 2018)
- Asynchronous Advantage Actor Critic (**A3C**) (Mnih et al. 2016)
- Syncrhonous Advantage Actor Critic (**A2C**)
- Proximal Policy Optimisation (**PPO**) (Schulman et al. 2017)
- DQN with Hindsight Experience Replay (**DQN-HER**) (Andrychowicz et al. 2018)
- DDPG with Hindsight Experience Replay (**DDPG-HER**) (Andrychowicz et al. 2018 )
- Hierarchical-DQN (**h-DQN**) (Kulkarni et al. 2016)
- Stochastic NNs for Hierarchical Reinforcement Learning (**SNN-HRL**) (Florensa et al. 2017)
- Diversity Is All You Need (**DIAYN**) (Eyensbach et al. 2018)

### PFRL

PFRL is a deep reinforcement learning library that implements various state-of-the-art deep reinforcement algorithms in Python usingÂ [PyTorch](https://github.com/pytorch/pytorch).

Github åœ°å€ï¼š[GitHub - pfnet/pfrl: PFRL: a PyTorch-based deep reinforcement learning library](https://github.com/pfnet/pfrl)

å®ç°çš„  RL Algorithms

| Algorithm                      | Discrete Action | Continous Action | Recurrent Model | Batch Training | CPU Async Training | Pretrained models* |
| ------------------------------ | --------------- | ---------------- | --------------- | -------------- | ------------------ | ------------------ |
| DQN (including DoubleDQN etc.) | âœ“               | âœ“ (NAF)          | âœ“               | âœ“              | x                  | âœ“                  |
| Categorical DQN                | âœ“               | x                | âœ“               | âœ“              | x                  | x                  |
| Rainbow                        | âœ“               | x                | âœ“               | âœ“              | x                  | âœ“                  |
| IQN                            | âœ“               | x                | âœ“               | âœ“              | x                  | âœ“                  |
| DDPG                           | x               | âœ“                | x               | âœ“              | x                  | âœ“                  |
| A3C                            | âœ“               | âœ“                | âœ“               | âœ“ (A2C)        | âœ“                  | âœ“                  |
| ACER                           | âœ“               | âœ“                | âœ“               | x              | âœ“                  | x                  |
| PPO                            | âœ“               | âœ“                | âœ“               | âœ“              | x                  | âœ“                  |
| TRPO                           | âœ“               | âœ“                | âœ“               | âœ“              | x                  | âœ“                  |
| TD3                            | x               | âœ“                | x               | âœ“              | x                  | âœ“                  |
| SAC                            | x               | âœ“                | x               | âœ“              | x                  | âœ“                  |

### æ¸…åå¤©æˆï¼ˆTianshouï¼‰

å¤©æˆï¼ˆTianshouï¼‰æ˜¯çº¯ åŸºäº PyTorch ä»£ç çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸ç›®å‰ç°æœ‰åŸºäº TensorFlow çš„å¼ºåŒ–å­¦ä¹ åº“ä¸åŒï¼Œå¤©æˆçš„ç±»ç»§æ‰¿å¹¶ä¸å¤æ‚ï¼ŒAPI ä¹Ÿä¸æ˜¯å¾ˆç¹çã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå¤©æˆçš„è®­ç»ƒé€Ÿåº¦éå¸¸å¿«ï¼Œæˆ‘ä»¬è¯•ç”¨ Pythonic çš„ API å°±èƒ½å¿«é€Ÿæ„å»ºä¸è®­ç»ƒ RL [æ™ºèƒ½ä½“]()ã€‚

Tianshouçš„ä¼˜åŠ¿ï¼š

- å®ç°ç®€æ´
- é€Ÿåº¦å¿«
- æ¨¡å—åŒ–
- å¯å¤ç°æ€§

ç›®å‰å¤©æˆæ”¯æŒçš„ RL ç®—æ³•æœ‰å¦‚ä¸‹å‡ ç§ï¼š

- Policy Gradient (PG)
- Deep Q-Network (DQN)
- Double DQN (DDQN) with n-step returns
- Advantage Actor-Critic (A2C)
- Deep Deterministic Policy Gradient (DDPG)
- Proximal Policy Optimization (PPO)
- Twin Delayed DDPG (TD3)
- Soft Actor-Critic (SAC)

å¦å¤–ï¼Œå¯¹äºä»¥ä¸Šä»£ç å¤©æˆè¿˜æ”¯æŒå¹¶è¡Œæ”¶é›†æ ·æœ¬ï¼Œå¹¶ä¸”æ‰€æœ‰ç®—æ³•å‡ç»Ÿä¸€æ”¹å†™ä¸ºåŸºäº replay-buffer çš„å½¢å¼ã€‚

github åœ°å€ï¼š[https://github.com/thu-ml/tianshou](https://github.com/thu-ml/tianshou)

## Tutorial

An Introduction to Reinforcement Learning Using OpenAI Gym
<https://www.gocoder.one/blog/rl-tutorial-with-openai-gym>

An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab
<https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google>

Intro to RLlib: Example Environments
<https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70>

Ray and RLlib for Fast and Parallel Reinforcement Learning
<https://towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c>



## çŸ¥ä¹ä¸“æ 

- [å¼ºåŒ–å­¦ä¹ çŸ¥è¯†å¤§è®²å ‚](https://zhuanlan.zhihu.com/sharerl)

- è¯¥ä¸“æ ä½œè€…å³ä¸ºã€Š[æ·±å…¥æµ…å‡ºå¼ºåŒ–å­¦ä¹ ï¼šåŸç†å…¥é—¨]()ã€‹ä¸€ä¹¦çš„ä½œè€…ï¼Œä¸“æ çš„è®²è§£åŒ…æ‹¬ï¼šå…¥é—¨ç¯‡ã€è¿›é˜¶ç¯‡ã€å‰æ²¿ç¯‡å’Œå®è·µç¯‡ï¼Œæ·±å…¥æµ…å‡ºï¼Œå†…å®¹ç¿”å®ï¼Œæ˜¯ä¸“é—¨é’ˆå¯¹å¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†å¤§è®²å ‚ã€‚

- [æ™ºèƒ½å•å…ƒ](https://zhuanlan.zhihu.com/intelligentunit)

- è¯¥ä¸“æ æ¶µç›–çš„å†…å®¹è¾ƒå¹¿ï¼Œä¸»è¦åŒ…æ‹¬æ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ åŠå…¶ç›¸åº”çš„å®è·µåº”ç”¨ï¼Œæ˜¯çŸ¥ä¹ä¸Šæ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ é¢†åŸŸå…³æ³¨é‡æœ€å¤§çš„ä¸“æ ï¼Œå…¶ä¸­å¯¹å¼ºåŒ–å­¦ä¹ çš„ä»‹ç»ä¹Ÿè¾ƒæµ…æ˜¾æ˜“æ‡‚ã€‚

- [ç¥ç»ç½‘ç»œä¸å¼ºåŒ–å­¦ä¹ ](https://zhuanlan.zhihu.com/c_101836530)

- è¯¥ä¸“æ ä¸»è¦æ˜¯ä½œè€…å…³äºå¼ºåŒ–å­¦ä¹ ç»å…¸å…¥é—¨ä¹¦ç±ã€ŠReinforcement Learning : An introductionã€‹çš„è¯»ä¹¦ç¬”è®°ï¼Œå› æ­¤ï¼Œéå¸¸é€‚åˆåœ¨å•ƒè¯¥ä¹¦çš„æ—¶å€™å‚è€ƒè¯¥ä¸“æ ï¼Œä»¥æœ‰æ›´æ·±å…¥çš„ç†è§£ã€‚



## å¼ºåŒ–å­¦ä¹ ç®—æ³•å­¦ä¹ æµç¨‹æ¦‚è§ˆï¼š
### ç¦»æ•£åŠ¨ä½œï¼šï¼ˆValue Gradient ï¼‰
Q-table-learning -> DQN(Deep Q NetWork) -> Double DQN -> Dueling DQN ->Double Dueling DQN (D3QN) ->Twin Delayed DDPG (TD3 è¿ç»­åŠ¨ä½œ)

### è¿ç»­åŠ¨ä½œï¼šï¼ˆPolicy Gradientï¼‰

Actor-Critic -> Advantage Actor-Critic(A2C) -> Asynchronus A2C (A3C) -> Deep Deterministic Policy Gradient (DDPG) -> Distributed Distributional DDPG (D4PG) -> Soft Actor-Critic (SAC) -> Trust Region Policy Optimization (TRPO) -> Generalized Advantage Estimation (GAE) -> Proximal Policy Optimization(PPO)
