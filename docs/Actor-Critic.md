在[强化学习(十三) 策略梯度(Policy Gradient)](https://www.cnblogs.com/pinard/p/10137696.html)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。

在本篇我们讨论策略(Policy Based)和价值(Value Based)相结合的方法：Actor-Critic算法。

本文主要参考了Sutton的强化学习书第13章和UCL强化学习讲义的第7讲。

# 1. Actor-Critic算法简介

Actor-Critic从名字上看包括两部分，演员(Actor)和评价者(Critic)。其中Actor使用我们上一节讲到的策略函数，负责生成动作(Action)并和环境交互。而Critic使用我们之前讲到了的价值函数，负责评估Actor的表现，并指导Actor下一阶段的动作。

回想我们上一篇的策略梯度，策略函数就是我们的Actor，但是那里是没有Critic的，我们当时使用了蒙特卡罗法来计算每一步的价值部分替代了Critic的功能，但是场景比较受限。因此现在我们使用类似DQN中用的价值函数来替代蒙特卡罗法，作为一个比较通用的Critic。也就是说在Actor-Critic算法中，我们需要做两组近似，第一组是策略函数的近似：

𝜋𝜃(𝑠,𝑎)=𝑃(𝑎|𝑠,𝜃)≈𝜋(𝑎|𝑠)πθ(s,a)=P(a|s,θ)≈π(a|s)

第二组是价值函数的近似，对于状态价值和动作价值函数分别是：

𝑣̂ (𝑠,𝑤)≈𝑣𝜋(𝑠)v^(s,w)≈vπ(s)

𝑞̂ (𝑠,𝑎,𝑤)≈𝑞𝜋(𝑠,𝑎)q^(s,a,w)≈qπ(s,a)

对于我们上一节讲到的蒙特卡罗策略梯度reinforce算法，我们需要进行改造才能变成Actor-Critic算法。

首先，在蒙特卡罗策略梯度reinforce算法中，我们的策略的参数更新公式是：

𝜃=𝜃+𝛼∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑠𝑡,𝑎𝑡)𝑣𝑡θ=θ+α∇θlogπθ(st,at)vt

梯度更新部分中，∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑠𝑡,𝑎𝑡)∇θlogπθ(st,at)是我们的分值函数，不用动，要变成Actor的话改动的是𝑣𝑡vt，这块不能再使用蒙特卡罗法来得到，而应该从Critic得到。

而对于Critic来说，这块是新的，不过我们完全可以参考之前DQN的做法，即用一个Q网络来做为Critic, 这个Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。

现在我们汇总来说，就是Critic通过Q网络计算状态的最优价值𝑣𝑡vt, 而Actor利用𝑣𝑡vt这个最优价值迭代更新策略函数的参数𝜃θ,进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数𝑤w, 在后面Critic会使用新的网络参数𝑤w来帮Actor计算状态的最优价值𝑣𝑡vt。

# 2. Actor-Critic算法可选形式

在上一节我们已经对Actor-Critic算法的流程做了一个初步的总结，不过有一个可以注意的点就是，我们对于Critic评估的点选择是和上一篇策略梯度一样的状态价值𝑣𝑡vt,实际上，我们还可以选择很多其他的指标来做为Critic的评估点。而目前可以使用的Actor-Critic评估点主要有：

a) 基于状态价值：这是我们上一节使用的评估点，这样Actor的策略函数参数更新的法公式是：

𝜃=𝜃+𝛼∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑠𝑡,𝑎𝑡)𝑉(𝑠,𝑤)θ=θ+α∇θlogπθ(st,at)V(s,w)

b) 基于动作价值：在DQN中，我们一般使用的都是动作价值函数Q来做价值评估，这样Actor的策略函数参数更新的法公式是：

𝜃=𝜃+𝛼∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑠𝑡,𝑎𝑡)𝑄(𝑠,𝑎,𝑤)θ=θ+α∇θlogπθ(st,at)Q(s,a,w)

c) 基于TD误差：在[强化学习（五）用时序差分法（TD）求解](https://www.cnblogs.com/pinard/p/9529828.html)中，我们讲到了TD误差，它的表达式是𝛿(𝑡)=𝑅𝑡+1+𝛾𝑉(𝑆𝑡+1)−𝑉(𝑆𝑡)δ(t)=Rt+1+γV(St+1)−V(St)或者𝛿(𝑡)=𝑅𝑡+1+𝛾𝑄(𝑆𝑡+1，𝐴𝑡+1)−𝑄(𝑆𝑡,𝐴𝑡)δ(t)=Rt+1+γQ(St+1，At+1)−Q(St,At), 这样Actor的策略函数参数更新的法公式是：

𝜃=𝜃+𝛼∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑠𝑡,𝑎𝑡)𝛿(𝑡)θ=θ+α∇θlogπθ(st,at)δ(t)

d) 基于优势函数：在[强化学习(十二) Dueling DQN](https://www.cnblogs.com/pinard/p/9923859.html)中，我们讲到过优势函数A的定义：𝐴(𝑆,𝐴,𝑤,𝛽)=𝑄(𝑆,𝐴,𝑤,𝛼,𝛽)−𝑉(𝑆,𝑤,𝛼)A(S,A,w,β)=Q(S,A,w,α,β)−V(S,w,α),即动作价值函数和状态价值函数的差值。这样Actor的策略函数参数更新的法公式是：

𝜃=𝜃+𝛼∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑠𝑡,𝑎𝑡)𝐴(𝑆,𝐴,𝑤,𝛽)θ=θ+α∇θlogπθ(st,at)A(S,A,w,β)

e) 基于TD(𝜆λ)误差：一般都是基于后向TD(𝜆λ)误差, 在[强化学习（五）用时序差分法（TD）求解](https://www.cnblogs.com/pinard/p/9529828.html)中也有讲到，是TD误差和效用迹E的乘积。这样Actor的策略函数参数更新的法公式是：

𝜃=𝜃+𝛼∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑠𝑡,𝑎𝑡)𝛿(𝑡)𝐸(𝑡)θ=θ+α∇θlogπθ(st,at)δ(t)E(t)

对于Critic本身的模型参数𝑤w，一般都是使用均方误差损失函数来做做迭代更新，类似之前DQN系列中所讲的迭代方法. 如果我们使用的是最简单的线性Q函数，比如𝑄(𝑠,𝑎,𝑤)=𝜙(𝑠,𝑎)𝑇𝑤Q(s,a,w)=ϕ(s,a)Tw,则Critic本身的模型参数𝑤w的更新公式可以表示为：

𝛿=𝑅𝑡+1+𝛾𝑄(𝑆𝑡+1，𝐴𝑡+1)−𝑄(𝑆𝑡,𝐴𝑡)δ=Rt+1+γQ(St+1，At+1)−Q(St,At)

𝑤=𝑤+𝛽𝛿𝜙(𝑠,𝑎)w=w+βδϕ(s,a)

通过对均方误差损失函数求导可以很容易的得到上式。当然实际应用中，我们一般不使用线性Q函数，而使用神经网络表示状态和Q值的关系。

# 3. Actor-Critic算法流程

这里给一个Actor-Critic算法的流程总结，评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用神经网络来更新网络参数

算法输入：迭代轮数𝑇T，状态特征维度𝑛n, 动作集𝐴A, 步长𝛼,𝛽α,β，衰减因子𝛾γ, 探索率𝜖ϵ, Critic网络结构和Actor网络结构。

输出：Actor 网络参数𝜃θ, Critic网络参数𝑤w

1. 随机初始化所有的状态和动作对应的价值𝑄Q. 随机初始化Critic网络的所有参数$w$。随机初始化Actor网络的所有参数$\theta$。

2. for i from 1 to T，进行迭代。

a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量𝜙(𝑆)ϕ(S)

b) 在Actor网络中使用𝜙(𝑆)ϕ(S)作为输入，输出动作𝐴A,基于动作𝐴A得到新的状态𝑆′S′,反馈𝑅R。

c) 在Critic网络中分别使用𝜙(𝑆)，𝜙(𝑆‘′)ϕ(S)，ϕ(S‘′)作为输入，得到Q值输出𝑉(𝑆)，𝑉(𝑆′)V(S)，V(S′)

d) 计算TD误差𝛿=𝑅+𝛾𝑉(𝑆′)−𝑉(𝑆)δ=R+γV(S′)−V(S)

e) 使用均方差损失函数∑(𝑅+𝛾𝑉(𝑆′)−𝑉(𝑆,𝑤))2∑(R+γV(S′)−V(S,w))2作Critic网络参数𝑤w的梯度更新

f) 更新Actor网络参数𝜃θ:

𝜃=𝜃+𝛼∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑆𝑡,𝐴)𝛿θ=θ+α∇θlogπθ(St,A)δ

对于Actor的分值函数∇𝜃𝑙𝑜𝑔𝜋𝜃(𝑆𝑡,𝐴)∇θlogπθ(St,A),可以选择softmax或者高斯分值函数。

上述Actor-Critic算法已经是一个很好的算法框架，但是离实际应用还比较远。主要原因是这里有两个神经网络，都需要梯度更新，而且互相依赖。但是了解这个算法过程后，其他基于Actor-Critic的算法就好理解了。
