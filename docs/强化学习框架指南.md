## 强化学习训练流程Pipeline

一个最基本的深度强化学习训练流程 pipeline 应该是这样的：

1. 初始化环境、网络、经验池
2. 在环境中探索，并把数据存入经验池
3. 从经验池中取出数据，更新网络参数
4. 对训练得到的策略进行评估，循环 2、3、4步

```python
# initialization
env    = BuildEnv()
actor  = PolicyNetwork()
critic = ValueNetwork()
buffer = ExperimenceRelayBuffer()

# training loop
for i in range(training_episode):

    # explore in env
    state = env.reset()
    for _ in range(max_step):
        next_state, reward, done, info_dict = env.step(action)
        buffer.append((state, reward, done, next_state))  # transition
        state = next_state
        if done:
            break

    # update network parameters
    for _ in range(...):
        batch_data = buffer.random_sample()
        Q_label    = ...
        critic_loss = cirterion(Q_label, critic(...))  # loss function
        actor_loss  = criterion(state, actor(...))   # Q value estimation
        Optimizer(network_parameters, object, ...).backward()

    # evaluate the policy (NOT necessary)
    if i % time_gap == 0:
        episode_return = evaluate(env, actor)

    if stop_training:
        break

save_model(network_parameters)
```

大部分深度强化学习DRL 算法（主要是策略梯度 `policy gradient、Actor-Critic Methods`）可以抽象成上面这种 `DDPG-style RL training pipeline`。它的可拓展性非常好，且方便拓展，训练稳定。

> 大部分DRL算法，指的是 Off-policy的 DDPG、TD3、SAC 等，以及 On-policy 的 A3C、PPO 等 及其变体。大部分算法的区别只在于：计算Q值、探索环境罢了。如果是DQN类的，那么只需要把 actor 看成是 arg max (Q1, ..., Qn)，critic看成是Q Network 即可。

## 算法基类的可拓展性

```python
class AgentBaseAC:
    def __init__():
    def update_buffer():
    def update_policy():
    def select_action():
    def save_or_load_model():
```

下面对深度强化学习进行分类，并举出特例，这是为了解释抽象深度学习算法框架的合理性。加入新算法时，只需要继承AgentBaseAC这个基类，做出尽可能少的修改即可。只要遵守编写规范，新算法可以随意地切换到多进程，多GPU训练模式而不用修改代码。

## 算法基类：将「探索环境」与「更新参数」这两个步骤分开

任何DRL算法都有这两个步骤，将它们分开非常重要：

```python
def update_buffer():  # 在环境中探索，并把数据存入经验池
def update_policy():  # 从经验池中取出数据，更新网络参数

# 稳定，高效
for _ in range():
    update_buffer()
for _ in range():
    update_policy()

# 既不稳定，又不高效
for _ in range():
    update_buffer()
    update_policy()
```

两个步骤分开的优点：

- 稳定：往Replay里加入新东西会改变数据分布，将两个步骤分开后，随机抽样更稳定，更加适合使用「动量」更新的优化器。
- 高效：update_buffer需要与环境交互，用CPU计算env.step，用CPU或GPU计算policy function。update_policy整个过程移到GPU里更高效。
- 代码复用：很多DRL算法的update_buffer相同，只有update_policy不同。
- 多进程兼容：分开后，update_buffer 可以并行地与环境交互并更新replay buffer，update_policy 可以并行地计算梯度并更新参数。

## 算法基类：将「选择动作」独立出来

很多DRL算法都将「选择动作」独立出来：

```python
def select_action(state):
    ...
    return action
```

深度强化学习可分为 确定策略梯度`Deterministic PG` 与随机策略梯度 `Stochastic PG`。从工程实现的角度看：它们探索环境的方式不同。

- 确定策略会为action添加一个由人类指定的高斯噪声
- 随机策略会让 policy network为action 输出一个用于探索的noise
- 此外，DQN经常使用 epsilon-Greedy 作为作为探索手段，Noisy DQN 把noise挪到激活函数之前与SAC神似
- 因此，不同的DRL算法有不同的select_action。
- 在编写强化学习库时，将 select_action 这个动作从 update_buffer 中抽离出来，避免太大改动

> 随机策略会让训练network为action输出一个用于探索的noise，特例： 随机策略PPO的 action noise std 是一个 trainable parameter，而不是由policy network 输出的。当然可以修改PPO让它也像SAC一样“由网络输出 action std”.

## 算法基类：保存或加载模型

事实上，在深度强化学习中，需要时常地保存模型参数，因为DRL没有很好的判断过拟合的方法。因此将「保存或加载模型」这个方法写在算法基类中。

- 在有监督的深度学习中，将数据集划分为训练集、验证集、测试集。在训练集上训练，看到验证集的损失上升时，就停止训练，记下此时的超参数。
- 在深度强化学习中，并没有训练集、测试集之分。
- 在仿真环境（或者真实环境）中训练智能体，会得到一个分数（累计回报 episode return）。
- 将这个分布画出来，就得到学习曲线 learning curve。用这个曲线来判断何时终止训练。
- DRL算法并不是训练时间越长，得分越高，可以保存整个训练过程中，得分最高的策略模型。有时候环境过于复杂，重置环境后同一个策略会得到不同的分数，所以要多测几次让分数有统计意义（请注意，下面的折线图上每个点已经是多测几次得到的结果）。

像 OpenAI baseline 以及 hill-a/stable-baselines 缺少自动绘制 learning curve 辅助判断模型合适终止的模块，因此需要使用者自己去编写。

## 经验回放 Experience Replay Buffer

任何深度强化学习算法都需要 Replay，因为深度学习（神经网络）一定要稳定的数据才能训练。而将训练数据保存到 Buffer 里，然后随机抽样是让数据接近独立同分布的好方法。一个成熟的强化学习库一定会在这方面下功夫：复杂的环境需要管理大容量的 Buffer，并且整个训练流程有Buffer参与的部分都是高IO的操作。

```python
class BufferArray:
    def __init__():
    def append_memo():  # 保存单个 environment transition (s, a, r, next state)
    def extend_memo():  # 保存多个 多进程时，批量收集数据时用到
    def random_sample():  # 随机采样，off-policy会用到（on-policy算法也用，但不在这里用）
    def all_sample():     # 取出全部数据，on-policy算法会用到

    def update__now_len__before_sample():  # 更新指针（高性能Buffer）
    def empty_memories__before_explore():  # 清空所有记忆，on-policy算法会用到
```



经过试验，将训练数据buffer 放在连续内存上明显更快，对于任何DRL算法都是如此，所以抛弃了 list 结构，转而在使用前创建一块内存用于存放array，因此才会用到「更新指针」的这类操作。Replay Buffer 对整个DRL库的性能影响实在太大了，以至于我在这里为了性能牺牲了优雅。

深度强化学习可分为异策略 off-policy 与 同策略 on-policy。从工程实现的角度看：它们的Experimence Replay Buffer 的管理方式不同。

- 异策略的 Replay Buffer 里面可以存放来自于不同策略的 状态转移数据 state transition (s, a, r, mask, next state)，如果agent的探索能力强，那么Buffer 里面的数据多样性有保证，且不需要删除，直到达到内存的极限。

- 同策略的Buffer里面只能存放一种策略探索得到的数据，这种策略与它需要更新的策略相同。它需要在新一轮探索开始前，将更新参数时用过的数据删除掉。



mask 是什么？参见「合并 终止状态done 与 折扣因子gamma」

>  (s, a, r, mask, next state)，例外： 使用 轨迹 trajectory进行更新的算法，可以不保存 next state，转而保存当前动作的Log_prob 或者用于计算出 log_prob 的noise 将用过的数据删除掉，例外： 作为On-policy 的PPO算法当然要删除用过的训练数据，但是OpenAI基于PPG改进得到的PPG （PP Gradient）算法是一种能利用off-policy数据的On-policy算法。

深度强化学习可分为 以 state1D 或者 以 state2D 为输入。任何state都可以 flatten成 1D，因此在设计Buffer的时候，我可以将完整的 state transition（ state1D，reward，action1D）保存在一块连续的内存上，而不需要将它们分开保存，从而在 random sample 时极大地加快训练速度。以 state2D为输入的，如 雅达利游戏 Atatri Game 需要以画面截图作为输入 pixel-level state，有时候需要堆叠连续的几帧画面（视频），都直接把这些数据flatten 成一维的向量，统一保存，使用的时候再使用 reshape 进行复原。

基于以上两点，建议：

- 无论数据如何，全部都reshape成一维，然后统一保存在一块连续的内存上（或者直接保存在显存内），并且使用指针。训练时一定要保存到显存内，方便快速地使用随机抽样。
- 更新异策略时，保存  (s, a, r, mask, next state)。因为要进行随机抽样，所以一定要保存 next state。
- 更新同策略时，以 trajectory 形式 按顺序保存 (s, a, r, mask, noise)。每个state 的下一行就是 next state，因此不需要保存 next state。noise用于计算新旧策略的熵。

## 强化学习与深度学习的区别

这里从高性能计算的角度讲一下她们的区别：

有监督的深度学习（如：在ImagNet上使用监督数据训练分类器）。这个过程天生适合分布式，不同GPU（或设备）之间可以只传递梯度（中心 或者 环式），可以用多CPU加快数据读取：

1. 从磁盘中读取数据，存放到内存（可使用多进程加速，CPU workers）
2. 对数据进行预处理，并传入GPU的显存
3. random sample，在GPU里计算梯度，更新网络参数
4. 循环以上内容，定时地保存check point
5. 适时地终止训练，避免过拟合

深度强化学习（如：DDPG-style training pipeline）。DRL难以套用有监督DL的多进程加速方案，他们只有2、3步骤相同。在DL里，数据可以提前准备好，而DRL的数据需要与环境交互产生，并且需要严格限制交互次数。在DL里，我可以用训练的副产物 loss function 帮助我判断何时可以终止训练，避免过拟合，而DRL没有判断过拟合的机制，因此一定需要绘制出 学习曲线 帮助决定“何时终止训练”与“保存哪个策略”。

1. agent与环境交互，得到的零碎数据存放在内存中（一般是CPU，或者再加上GPU）
2. 将数据输入传入GPU的显存中
3. random sample，在GPU里计算梯度，更新网络参数
4. 对策略进行评估，绘制学习曲线，保存得分高的
5. 观察学习曲线，若分数不能更高，则终止训练



## 稳定训练

为了稳定训练，将训练流程分为三部分：「探索环境」、「更新参数」以及「评估模型」

前面提及的：将「探索环境」与「更新参数」这两个步骤分开，不仅能方便多进程的DRL训练，更重要的意义是：给「更新参数」步骤创造了数据稳定的训练环境，并且可以灵活调整**数据复用次数**，尽可能避免过拟合，从而稳定了DRL的训练。众所周知，像对抗网络、策略梯度这种双层优化结构的训练不稳定。**为了追求训练快速而舍弃泛化和稳定是不可取的**。

将「评估模型」也独立出来。**由于DRL并非训练越久模型越好**，只有在环境简单，算法给力、调参充分（甚至是精心挑选）的情况下才能得到那种漂亮的学习曲线。评估模型可以帮助修改训练环境，调整DRL超参数。**评估模型可以帮助修改训练环境，调整DRL超参数**，很多DRL库没有这个极其重要的部分。

![img](https://pic1.zhimg.com/80/v2-6c6177b7764e84f86978fa13c4e133af_1440w.jpg?source=d16d100b)



- 「更新参数」DRL库的设计原则是：绝不阻塞主进程。因此将「探索环境」与「评估模型」分了出去。「更新参数」与「探索环境」两个进程会轮流使用GPU，绝不让GPU闲着。

- 「探索环境」进程会把探索得到的数据通过管道发送给「更新参数」进程，为了降低GPU的空闲率，采用了一种新的采样模式。

- 评估模型」是比较独立的进程，它将会利用最零碎的资源去完成记录任务，优先级最低。

  例如主进程会把需要评估的模型（actor network）发送给它，暂存在队列里。如果它来不及评估这个模型，而主进程又发来一个新的模型，那么它会在上一次的评估结束后，直接读取最新的模型：**主进程不需要等待它**，有评估任务它就做，没有任务它就等，并且它只使用CPU，绝不占用宝贵的GPU资源。它还负责保存模型到硬盘、记录训练的临时变量的折线图，有助于在训练崩溃时定位错误、在复盘的时候调整超参数。。可以被监视的部分临时变量：

  - 智能体在环境中每轮训练的步数（均值、方差）

  - ReplayBuffer 内记忆的数量

  - DQN类、Actor-critic类：objectives of Q Network/Critic/Actor、Q值

  - TD3：TwinCritc 的两个Q值差值的方差，动作与最近边界的距离

  - PPO：搜索得到的动作噪声方差，新旧策略的差异，clip前后两个objective差值的方差

  - SAC：策略的熵，温度系数alpha的值，动作与最近边界的距离

  即便绘制折线图不会影响到主进程的训练，但是从主进程采集并传输临时变量依然会拖慢训练速度。

## Env修改建议

下面是一个勉强可以在 2020年最好的DRL库 [RLlib ray-project](http://github.com/ray-project/ray) 以及比OpenAI baselines 稳定一点点的 [stable-baselines](http://github.com/hill-a/stable-baselines) 使用的环境。如果你想要使用 RLlib ray 的多进程、分布式计算等其他功能，你需要自行阅读他们官网的Document、和Github的Demo。

```python
import gym
class DemoGymEnv(gym.Env):
    def __init__(self):
        self.observation_space = gym.spaces.Box(low=-np.inf, high=+np.inf, shape=(4,))  # state_dim = 4
        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))  # action_dim = 2

    def reset(self):
        state = rd.randn(4)  # for example
        return state

    def step(self, action):
        state = rd.randn(4)  # for example
        reward = 1
        done = False
        return state, reward, done, dict()  # gym.Env requires it to be a dict, even an empty dict

    def render(self, mode='human'):
        pass
```

一些知名的DRL库需要gym只是为了规范化 state 和 action，比如 gym.spaces.Box。env的建立，不需要OpenAI 的gym库，只要能告诉DRL库：state_dim 和 action_dim 这些信息。一个合格的环境只需要有 reset、step这两个方法，没那么复杂，然后直接在init里写上环境的信息，方便DRL库根据具体任务创建合适的网络。 target_reward 是目标分数，达到了目标分数后就视为通关了，不清楚的情况下可以随便写一个数值。

```python
class DemoEnv:
    def __init__(self,):
        self.env_name = 'DemoEnv-v1'
        self.state_dim = int(...)
        self.action_dim = int(...)
        self.if_discrete = bool(...)
        self.target_reward = float(...)

    def reset(self):
        ...
        return state

    def step(self, actions):
        ...
        return state, reward, done, None
```



## 半精度（很容易做成 switch on/off 模式）

DRL和半精度相性非常好，可以做到在网络内部全称使用半精度，天作之和：

- 强化学习需要快速拟合而不能用深层网络>10，半精度也在深层网络容易梯度消失。
- 强化学习和批归一化不兼容（写于2020年），半精度在计算批归一化的时候也需要转换回float32。

可以用简单的几行代码实现，因此可以做成 switch on/off 模式，GPU有 TensorCore 的情况下提速很快。此外，只有用GPU才有必要用半精度。「评估模型」辅线程不需要用GPU，因此也不需要用半精度（甚至因为CPU的制程，float64改为float32都不会加快速度，只会节省内存）

## 【高性能的DRL库细节】

### 合并 终止状态done 与 折扣因子gamma

下面这些细节，只改进一处地方，不一定都会有肉眼看得见的性能提升。但如果全部都改了，其性能提升会非常明显。合并 终止状态done 与 折扣因子gamma

有很大改进空间的旧方法：

```python
next_state, reward, done, info_dict = env.step(action)
assert isinstance(done, bool)
assert isinstance(gamma, float)

q_value = reward + (1-float(done)) * gamma * q_next
```

合并 终止状态done 与 折扣因子gamma，**使用mask代替**。保存在 Replay Buffer中的是 mask，不再保存 float(done)。修改后，不需要 float(done)，不需要减法，不需要乘以两个数：

```python
mask = gamma if done else 0.0

q_value = reward + mask * q_next
```

### 将Buffer保存在一块连续的内存上

比如PyTorch 官网提供的强化学习 Buffer保存例子（2020年），使用了 NamedTuple，此外还有其他方案，如下：

```python
Method       Used Times (second)   Detail
List         24                    list()
NamedTuple   20                    collections.namedtuple
Array(CPU)   13                    numpy.array/torch.tensor(CPU)
Array(GPU)   13                    torch.tensor (GPU)
```

![img](https://pic1.zhimg.com/80/v2-95512ebba134f3c5ec998617a5b8750e_1440w.jpg?source=d16d100b)



## 高性能的PyTorch

【如何高效地将CPU数组转化成GPU里的张量？】只能用这种方法，它最快：

```python
device = torch.device('cuda')
reward = 1.1
mask = 0.0 if done else gamma
state = np.array((2.2, 3.3), dtype=np.float32)
action = np.array((4.4, 5.5), dtype=np.float32)

array = np.hstack((reward, mask, state, action))
tensor = torch.tensor(array, dtype=torch.float32).cuda()  # slowest and bad
tensor = torch.tensor(array, dtype=torch.float32).to(device)  # slower
tensor = torch.tensor(array, dtype=torch.float32, device=device)
tensor = torch.as_tensor(array, dtype=torch.float32, device=device)  # faster

tensor = torch.as_tensor(array, device=device)  # fastest !!!!!!!!!!!!!!!!!!!!!!!!
tensor = torch.as_tensor((reward, mask, *state), device=device)  # slower
tensor = torch.from_numpy(array).to(device)  # slower


# 以下三种等效
tensor = net(tensor.detach())
tensor = net(tensor).detach()
with torch.no_grad():
    tensor = net(tensor)
```
