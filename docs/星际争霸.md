# 基于多智能体强化学习主宰星际争霸游戏

大家好，今天我们来介绍基于多智能体强化学习主宰星际争霸游戏这篇论文

[Grandmaster level in StarCraft II using multi-agent reinforcement learningdoi.org/10.1038/s41586-019-1724-z](https://link.zhihu.com/?target=https%3A//doi.org/10.1038/s41586-019-1724-z)

从Alphastar以后 利用强化学习的方法进行星际争霸2AI的又一大突破。

## Part1 前言

### 游戏介绍(Introduction to the Game)

### 游戏简介

   既然是打星际争霸的论文，那么我们首先看下什么是星际争霸

《星际争霸II》是一个实时进行的战略游戏。你可以从上帝视角观察和指挥部队，争夺战场的控制权，并最终击败你的对手。游戏的单人游戏战役主要聚焦于人类种族，但你可以在多人游戏比赛中使用以下三个《星际争霸II》的种族的任意一个：**人类、星灵和异虫**。

每个种族都拥有其独特的单位；这些单位在战场上各自有其特定的角色。结合不同的单位来组成一支多功能的部队，是走向胜利的常规道路之一。

你能指挥的最基础的单位是工人。他们会采集资源，让你用来扩张基地，以及为你日渐庞大的部队招兵买马，同时他们也能建造新的建筑。一些高级单位在你的基地达到特定要求之后就会解锁，你需要建造特定建筑，或者研究相应的科技。

在[多人游戏](https://www.zhihu.com/search?q=多人游戏&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})比赛中，如果你将地图上敌人的所有建筑都摧毁，或者敌人向你投降，那么你就**赢得了比赛**。

### 视频介绍

<iframe title="video" src="https://video.zhihu.com/video/1200527176032047104?player=%7B%22autoplay%22%3Afalse%2C%22shouldShowPageFullScreenButton%22%3Atrue%7D" allowfullscreen="" frameborder="0" class="css-uwwqev"></iframe>

星际争霸介绍

总结来说:

1.《星际争霸2》是一款RTS（即时战略）游戏，说白了就是造农民采矿、造建筑、造兵、攀科技，最后派兵拆光对手的建筑。

2.与围棋相比，虽然都属于零和博弈，但还多了不完全信息(比如对手的信息是看不到的)、输入输出状态空间更庞大、存在海量先验信息、游戏预测困难的问题。

### 重大挑战与解决方案（Major challenges and solutions)

下面我们分别从游戏层面和算法层面解释这篇论文之前，强化学习解决星际争霸2AI遇到了哪些问题，以及这篇论文所做出的改进，箭头前面表示问题后面就是该论文的解决方案与改进。

### 游戏层面

- 群体博弈 -> League Learning

>  星际争霸2是一个多人即时战略游戏，所以涉及到群体博弈。
>

- 不完全信息 -> LSTM

>  不完全信息,对手的视野我们无法看到
>

- 长期规划/回报稀疏 -> TD(λ) & UGPO

>  一局游戏需要几万步决策，但是最后只有一个稀疏的奖励
>

- 实时控制(APM有限值)-> Monitoring layer

>  对于操作速度（action per minute，APM）有限制,要接近人的操作速度
>

- 超大状态空间 -> self-attention & scatter connection

>  由于是end to end 的训练方法，所以该模型的状态输入是每一帧图像。
>

- 超大动作空间 -> auto-regressive policy

>  动作空间组合数目较多，每一个动作都需要先选择一个对象（比如农民），选择动作的类型，然后可能还需要地图中选择作用的位置（比如走到某个位置），最后还需要选择什么时候进行下一个动作。
>

### 算法层面

- 自博弈循环 ->PFSP

>    对于算法层面如果采用自博弈（自己和自己玩）的方法来学习与更新策略的话，随着学习的进行，可能会出现循环。类似[有向图](https://www.zhihu.com/search?q=有向图&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})中的闭环
>

- 自博弈偏移 ->baseline condition (z)

>  如果纯使用自博弈来学习，学习到的策略可能不能有效对抗人类策略。
>

### 模型构建(Model Building)

先来给大家一个直观的感受，我们的智能体到底输入了(看到了什么)以及它是怎么认知的，以及最后他输出了什么。

![动图封面](https://pic1.zhimg.com/v2-b8aa7666612fb0bd9febbffd4ce01e58_b.jpg)

<video class="ztext-gif GifPlayer-gif2mp4 css-1xeqk96" src="https://vdn6.vzuu.com/SD/54d46478-ec6c-11ea-acfd-5ab503a75443.mp4?pkey=AAXaYOED5jGuef79yieFUAO9lrkjkgWE3hYvaqmwaa7Qe-ODfFi2xw-AnJvHFAaqD3b0FpPNgFj1_YsKWtytmB-y&amp;c=avc.0.0&amp;f=mp4&amp;pu=078babd7&amp;bu=078babd7&amp;expiration=1660833879&amp;v=ks6" data-thumbnail="https://pic1.zhimg.com/v2-b8aa7666612fb0bd9febbffd4ce01e58_b.jpg" poster="https://pic1.zhimg.com/v2-b8aa7666612fb0bd9febbffd4ce01e58_b.jpg" data-size="normal" preload="metadata" loop="" playsinline=""></video>



1. 首先输入的是小地图图像以及当前所有的兵种信息(这也是我在打星际的时候能看到的信息)
2. 通过神经网络层之后输出动作信息, (右下角就是他们选中的动作)其中包括选中谁、去哪里、去干什么等动作

下面我们来看一下整个模型的网络结构。

## Part 2 网络结构

### 网络结构介绍

![img](https://pic1.zhimg.com/v2-40ef073861c2b0aebc3523f38f3e2854_b.jpg)网络结构

-  整体上使用AC框架，与强化学习中使用AC的Cartpole对比，Alphastar不是一般的将输入放在一起输入一个CNN，而是对于不同的输入类型，进行不同的操作，经过不同的网络输出。

-  总的来看，输入有基准特征(baseline features)，标量特征(scaler features)，实体列表(entities)和minimap组成。

-  值函数网络输出数值，而策略网络输出了五项内容。他们具体是什么，我们接下来具体分析。

-  Alphastar的网络结构设计的很精妙。我认为他的设计思路是

- 对于一般特征的提取就用多层感知器
- 对于图像信息的处理，它使用计算机视觉中常用的特征提取网络比如残差网络
- 在所有用到了entities list的地方，它很巧妙的利用了entities序列和句子序列的相似，使用NLP中处理句子的模型提取实体列表的特征。

### 价值函数网络

![img](https://pic3.zhimg.com/v2-7c0eae34f222d01e40e9df7343901c16_b.jpg)价值网络



-  z：论文中称它为statistic vector，从代码来看应该是从不同人类玩家l历史数据中抽取出来的建造顺序，以及单位、建筑、升级的顺序等信息。

-  Sc2的特点是Long term strategy 对于初始状态很敏感，联系密切。单独学习人类前20个建造物顺序可以大大减小可能的状态动作空间，加速收敛。

-  此外，从结果分析可以看出，加入观测的对手信息效果提升明显。

-  Baseline feature的概念受限于本人水平有限，加上论文中并没有详细说明。我认为有两种解释

- 引用的是Counterfactual Multi-Agent Policy Gradients论文中的反事实基线的概念，具体可以参见PPT中的COMA论文
- baseline中的base是指以人类为基础进行学习

### 策略函数网络

![img](https://pic4.zhimg.com/v2-879fd0ee3792d72d32a92070dd79ab4b_b.jpg)策略网络



- **注意：Alphastar不看主屏幕，只看小地图！**
- 小地图+单位列表+标量信息（时间，种族，科技情况，人口情况）已经包含了所有信息

![img](https://pic2.zhimg.com/v2-da01b33c928a5089bac9634b9fb19131_b.jpg)策略网络

- 不同于Alphago只有前几手的棋盘。Alphastar存储过去的所有信息，将每一个time_step输入到LSTM，赋予各个时间段observation和action时序性。
- 论文中的提到的 包含过去所有时刻的[obs](https://www.zhihu.com/search?q=obs&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})和action信息 就是 每个time step都按次序输入LSTM
- LSTM使得这个模型可以记住过去做的事情
- Eg: 一些建筑物只需要在初期建造， 即使它以后被拆除，（有了LSTM）即使entity list里面没有了，也不再建了
- 时间序贯性很重要
- Transorformer：不必多说，BERT肯定大家都听说过，非常火。在这里可以认为是输出了一个考虑了各个元素之间依赖的新的list。List的成员和输出词汇表（数据库）有关，没有查到相关资料

### **动作属性**

![img](https://pic3.zhimg.com/v2-1f8793c5e72731ac689666845114fce2_b.jpg)动作属性表



>  查阅开源的Pysc2平台文档，得知动作class下有5个属性。
>

### 框架总览

![img](https://pic1.zhimg.com/v2-72ec9a652006444577ab4a0117d47af0_b.jpg)总体框架



首先根据所有信息

1.  选择action type（what）

2.  通过MLP确定[delay](https://www.zhihu.com/search?q=delay&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})(when)

3.  同样使用MLP，确定是否要加入队列。因为APM有限制，有一些动作不能被立即执行，要加入Quene等待以后的时间执行。

4.  对于动作的执行者(Which)，使用Pointer Network，直接获取[entities](https://www.zhihu.com/search?q=entities&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})作为input，output一个pointer直接指向input的一个元素。至于为什么选择Pointer network，笔者一开始甚至没有听说过pointer  network。经过查阅资料，认为原因如下


>    Attention和[pointer](https://www.zhihu.com/search?q=pointer&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})类似，传统的带有注意力机制的seq2seq模型的运行过程是这样的，先使用encoder部分对输入序列进行编码，然后对编码后的向量做attention，最后使用[decoder](https://www.zhihu.com/search?q=decoder&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})部分对attention后的向量进行解码从而得到预测结果。但是作为Pointer Networks，得到预测结果的方式便是输出一个概率分布，也即所谓的指针。
>  ​  也就是说传统带有注意力机制的seq2seq模型输出的是针对输出词汇表的一个概率分布，而Pointer Networks输出的则是针对输入文本序列的概率分布。
>  ​  对于施法者选择，因为[candidates](https://www.zhihu.com/search?q=candidates&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})是inputs的子集，选择pointer Network更加高效。
>

1. 对于被执行者的选择(Who)，需要location或者entities的信息。对于entities序列，我们使用NLP方法处理，对于locations图片信息，我们使用CV方法处理。

下面我们来介绍该模型训练的方法

## Part 3 训练部分

​       AlphaStar的训练过程最概括的表示是：先进行**基于人类数据的监督学习**，然后再进行**使用联盟方法的强化学习**。

![img](https://pic2.zhimg.com/v2-19a02d136145f80a50098c3e452cb789_b.jpg)联盟学习图解



全图流程：**监督学习** →\to\to **通过匹配对抗(联盟学习)进行强化学习** →\to\to **AlphaStar**。

后文会对此进行更详细解释说明。

### 监督学习

AlphaStar的监督学习目标是：学习人类对**单位的操作**以及**建筑建造的操作**。其中建筑建造的单元和顺序需要单独列出来训练，因为很多情况下，一些操作必须要有一些建筑作为前提。

![img](https://pic4.zhimg.com/v2-93167cb4d162c89da909e8c0b074467b_b.jpg)监督学习



上图为论文给出的训练结构图。其中：

- $o_t$ 表示人类的观察数据（小地图数据）
- $z_t$ 代表着建筑操作的建造单元和建造顺序（阅读其[伪代码](https://www.zhihu.com/search?q=伪代码&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})可以发现，对于建筑单元和顺序的学习是有选择性的学习，在下文会对此进行解释）。
- ata_ta_t 代表人类的实际操作动作。

可以看出，AlphaStar会根据**小地图数据** oto_to_t **和建造操作数据** ztz_tz_t 行学习，并且以**人类的操作** ata_ta_t **作为标签**进行训练，得到合适的**输出** πtSL\pi_t^{SL}\pi_t^{SL} **的神经网络**。经过训练后，其得到的是**三个种族**的神经网络各一个。

通过阅读伪代码，我们还了解到一些新的细节。

1.  AlphaStar先使用较高技术的人类数据（MMR>3500），并且使用较高的学习（ lr=10−3lr=10^{-3}lr=10^{-3} ）率进行快速学习；然后使用极高技术的人类数据（MMR>6200），并且使用较低的学习率( lr=10−5lr=10^{-5}lr=10^{-5} )进行微调学习。

2.  进行监督训练时，使用交叉熵损失MLE和L2规则损失共同作为损失函数进行训练。


>  伪代码中， Loss=lossMLE+10−5·lossL2Loss=loss_{MLE} + 10^{-5} · loss_{L2}Loss=loss_{MLE} + 10^{-5} · loss_{L2} 。
>

1. 对于 ztz_tz_t ，其会选择性的学习。其具体过程是：首先，读取人类游戏回放数据，并且读取建筑单元build unit和建筑单元顺序build order数据，存入z列表；之后，其有一定概率**舍弃一些建筑单元和顺序的数据**。

>  在伪代码中，其读取建筑数据Bo（建筑顺序）和 BuBuBu （建筑单元），然后又初始化两个等长度的随机True-False数组 BoolBoBool_{Bo}Bool_{Bo} 和 BoolBuBool_{Bu}Bool_{Bu} ，然后分别将 BoolBuBool_{Bu}Bool_{Bu} =与 BuBuBu 相乘， BoolBoBool_{Bo}Bool_{Bo} 与 BoBoBo 相乘，作为最后输出的建造数据 zzz 。
>

### 每个智能体的强化学习

首先要注意的是，此部分所介绍的是每个智能体的强化学习。

AlphaStar的强化学习目标是：**基于AC框架**，通过不断的**与其他玩家（实际上是其他智能体或者自身，后面会有详细介绍）的对抗**来进行学习。

![img](https://pic2.zhimg.com/v2-7a83554bfe67a4c61f388eed0e720e39_b.jpg)强化学习



上图为给出的训练结构图，其中：

- zzz 代表人类的建造单元和建造顺序（防止智能体的建筑操作偏离人类情况）。
- oto_to_t 代表智能体观察到的信息（小地图图片）。
- ot′o_t'o_t' 代表对手的一些信息（用于辅助训练，防止偏差过大）。
- RtR_tR_t 代表用于训练奖励（由 rpr_pr_p 与 rTr_Tr_T 组合而成）。
- rpr_pr_p 代表伪奖励（其基于人类数据 zzz 生成，在后面会对其作用进行详细介绍）。
- rTr_Tr_T 代表最终的胜利/平局/失败的奖励（对抗结果）。

训练流程如下：AlphaStar会将自身**观察的数据信息** oto_to_t 输入**AC框架下的神经网络**（为了降低方差，其也会将对手的信息 ot′o_t'o_t' 输入网络），输出策略 πt\pi_t\pi_t 和值函数 VtV_tV_t 。

- 对于策略，其通过以**相比监督学习网络输出的策略** πtSL\pi_t^{SL}\pi_t^{SL} **的** KL\text{KL}\text{KL} **散度**、**基于R_t使用V-Trace和UPGO的方式更新**。
- 对于值函数，其通过**基于** RtR_tR_t **的TD(** λ\lambda\lambda **)方式更新**。

其中细节性的技术内容将在后文有详细介绍。

### AlphaStar的群强化学习

个人认为，群强化学习思想是AlphaStar最为核心的思想。

### 群强化学习

群强化学习的核心思想是：创建一个联盟League，联盟内有很多个体；通过不断的让联盟内部的个体之间相互对抗来进行强化学习训练，使得每个个体都能得到提升。经过不断的内部对抗，其得到的**不是一个极度强大的个体，而是一个十分强大的群体**。

### AlphaStar的联盟智能体

在AlphaStar的训练中，其创建了**四类**个体。四类个体分别为：主探索智能体（Main Agents）、主探索者（Main Exploiters）、联盟智能体（League Exploiters）和历史参数个体（Past Players）。

![img](https://pic1.zhimg.com/v2-de8a05585390fe811eb8dcb7354a1290_b.jpg)联盟智能体

-  主智能体 Main Agents\text{Main Agents}\text{Main Agents}


- - 意义：**最核心**的智能体，是最终输出的AlphaStar。


  -  特征：


  - - 对抗对手：全部历史个体、主探索者、主智能体。
    -  定期存储自身参数为一个player并加入到Past Players中。
    -  使用PFSP的方式匹配对手。

>   PFSP是一种为了高效率学习的匹配极值。在其伪代码中，每一个个体（包括历史个体与当前的三类智能体个体）都会记录与其他个体对抗的胜率，主智能体会优先选择胜率低的个体对抗。主观上的理解就是：与菜鸟对抗可能学不到什么知识，但是和大佬对抗就很可能学到新技巧。
>



- 主探索者 Main Exploiters\text{Main Exploiters}\text{Main Exploiters}


- -  意义：用于**寻找主智能体的弱点**。


  - 特征：

  - - 定期存储自身参数为一个player并加入到Past Players中。
    - 每存储一次自身参数，就会把自己还原为监督学习的参数。
    - 对抗对手：主智能体、主智能体的历史个体。

- 联盟智能体 League Exploiters\text{League Exploiters}\text{League Exploiters}

- - 意义：用于**寻找整个群体的弱点**。

  - 特征：

  - - 定期存储自身参数为一个player并加入到Past Players中。
    - 没存储一次自身参数，就有25%概率将自己还原为监督学习的参数。

  -  对抗对手：全部历史个体。


-  历史个体 Past Players\text{Past Players}\text{Past Players}


- - 意义：用于存储**智能体的历史参数**。


通过对伪代码的阅读，我们对联盟学习的框架进行了进一步的总结（实际上与上文中的框架图表达意义一致）：

![img](https://pic3.zhimg.com/v2-ac0bfb6172c49e3cb54f27ce47b41b16_b.jpg)



- 每个智能体的大小可以代表其强度（也可以理解为时间先后关系，越早越小）。
- 每个主智能体都会向Past Players中添加自身的历史参数。
- 主智能体会与主探索智能体、主智能体、全部历史参数个体对抗。
- 主探索者会与主智能体、主智能体的历史参数对抗；并且其每次存储参数都会还原自身参数为监督学习参数。
- 联盟探索者会与权力历史参数个体对抗；并且每次存储参数都有25%概率回还原自身参数为监督学习参数。

### AlphaStar联盟学习的技巧解释

1.  定期存储自身参数为一个player并加入到Past Player，并且使用PFSP方式学习：实际上，通过记录历史参数并与其对抗的方式，可以使**智能体克服追逐循环**的情况（即很容易出现A战胜B，B战胜C，C又战胜A）。

2.  联盟智能体和主探索智能体会将自身参数还原为监督学习的参数：因为智能体很可能会学习到一种很小众且有明显弱点的策略，但是其能够很好的击败一些智能体——即**偏离人类的认知**。因此需要对其参数进行还原。至于主智能体，其是最终的输出智能体，因此不能还原为监督学习的参数。

3.  智能体的匹配机制：智能体会**根据目标来匹配的对手**。主智能体是最终的输出目标，因此必然要有极强的能力，即能够打败全部的个体；主探索者是为了在人类的基础上找到主智能体的弱点，因此对手均为主智能体及其历史；联盟探索者是为了在人类的基础上找到系统（群体）的弱点，因此对手均为全部的历史参数。


>  实际上，通过对伪代码的阅读，我们发现匹配机制要更加复杂。如有兴趣建议阅读其伪代码中的匹配函数。
>  联盟学习中的匹配的方式是**为了更高效的学习**而不是简单的随机。比如，主探索者在匹配对手时，会先观察其对主智能体的胜率——如果较低，则其余主智能体对抗；否则其会选择主智能体的历史中胜率较低的个体对抗。
>

下面我们来介绍一下本文的技术细节与成果分析

## Part 4 技术细节 & 成果分析

首先我们回到论文中强化学习的流程示意图，看看这上面都写了哪些高级的技术：

![img](https://pic4.zhimg.com/v2-b5fa4c6a07e35de03079d1d6903dff5b_b.jpg)



### KL散度

首先论文中用了KL技术来将人类策略和智能体学习到的策略来进行比较。那么什么是KL呢？一下摘自百度百科：

>  [相对熵](https://www.zhihu.com/search?q=相对熵&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})（relative entropy），又被称为Kullback-Leibler散度（Kullback-Leibler divergence）或信息散度（information divergence），是两个概率分布（probability distribution）间差异的非对称性度量。在在信息理论中，相对熵等价于两个概率分布的[信息熵](https://www.zhihu.com/search?q=信息熵&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})（Shannon entropy）的差值。
>

还是不太懂。那么看一下KL散度的公式：

离散形式与连续形式

KL(P‖Q)=∑P(x)log⁡P(x)Q(x)KL(P‖Q)=∫P(x)log⁡P(x)Q(x)dx\begin{aligned}\mathrm{KL}(P \| Q)=\sum P(x) \log \frac{P(x)}{Q(x)} \\ \mathrm{KL}(P \| Q)=\int P(x) \log \frac{P(x)}{Q(x)} d x\end{aligned}\begin{aligned}\mathrm{KL}(P \| Q)=\sum P(x) \log \frac{P(x)}{Q(x)} \\ \mathrm{KL}(P \| Q)=\int P(x) \log \frac{P(x)}{Q(x)} d x\end{aligned}

结合百科上面的介绍，我们可以知道P(x)和Q(x)是两个分布，KL散度就是通过上面这两种形式的公式来衡量P(x)和Q(x)这两个分布的差异性。另外，KL散度还有一个性质：非负性，所以我们可以通过在数值上尽量减小KL散度的方式来使理论分布去逼近真实的分布。在AlphaStar中，理论分布是指通过神经网络输出的分布，真实分布是指人类的策略分布。

### TD(λ)

在得到累积回报之后，智能体是如何利用这些累积回报来更新值函数的呢？由流程图所示，论文中用了TD(λ)算法。TD(λ)的更新算法如下所示：

V(St)←V(St)+α(Gtλ−V(St))V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(G_{t}^{\lambda}-V\left(S_{t}\right)\right)V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(G_{t}^{\lambda}-V\left(S_{t}\right)\right)

其中

Gtλ=(1−λ)∑n=1∞λn−1Gt(n)Gt(n)=Rt+1+γRt+2+…+γn−1Rt+n+γnV(St+n) \begin{aligned}G_{t}^{\lambda}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t}^{(n)}\\G_{t}^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1} R_{t+n}+\gamma^{n} V\left(S_{t+n}\right)\end{aligned} \begin{aligned}G_{t}^{\lambda}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t}^{(n)}\\G_{t}^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1} R_{t+n}+\gamma^{n} V\left(S_{t+n}\right)\end{aligned}

可以看到，TD(λ)实际上是传统时间差分方法与蒙特卡洛方法的融合，当λ=0时，只有当前的状态值更新：

v(St)←V(St)+αδtv\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha \delta_{t} v\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha \delta_{t}

当λ=1时，值函数的更新总量与蒙特卡洛方法相同：

δ1+γδt−1+γ2δt+2+…+γT−1−1δT−1=Rt−1+γV(St+1)−V(St)+γRt+2+γ2V(St+2)−γV(St+1)+γRt+3+γ3V(St+3)−γ2V(St+2)⋮+γT−1−tRT+γT−1V(ST)−γT−1−tV(ST,1)=Rt−1+γRt+2+γ2Rt+3+…+γT−1−tRT−V(St)=Gt−V(St)\begin{array}{l}{\quad \delta_{1}+\gamma \delta_{t-1}+\gamma^{2} \delta_{t+2}+\ldots+\gamma^{T-1-1} \delta_{T-1}} \\{=R_{t-1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)} \\{+\gamma R_{t+2}+\gamma^{2} V\left(S_{t+2}\right)-\gamma V\left(S_{t+1}\right)} \\{\quad+\gamma R_{t+3}+\gamma^{3} V\left(S_{t+3}\right)-\gamma^{2} V\left(S_{t+2}\right)} \\{\quad \vdots} \\{\quad+\gamma^{T-1-t} R_{T}+\gamma^{T-1} V\left(S_{T}\right)-\gamma^{T-1-t} V\left(S_{T, 1}\right)} \\{=\quad R_{t-1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots+\gamma^{T-1-t} R_{T}-V\left(S_{t}\right)} \\{=\quad G_{t}-V\left(S_{t}\right)}\end{array}\begin{array}{l}{\quad \delta_{1}+\gamma \delta_{t-1}+\gamma^{2} \delta_{t+2}+\ldots+\gamma^{T-1-1} \delta_{T-1}} \\{=R_{t-1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)} \\{+\gamma R_{t+2}+\gamma^{2} V\left(S_{t+2}\right)-\gamma V\left(S_{t+1}\right)} \\{\quad+\gamma R_{t+3}+\gamma^{3} V\left(S_{t+3}\right)-\gamma^{2} V\left(S_{t+2}\right)} \\{\quad \vdots} \\{\quad+\gamma^{T-1-t} R_{T}+\gamma^{T-1} V\left(S_{T}\right)-\gamma^{T-1-t} V\left(S_{T, 1}\right)} \\{=\quad R_{t-1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots+\gamma^{T-1-t} R_{T}-V\left(S_{t}\right)} \\{=\quad G_{t}-V\left(S_{t}\right)}\end{array}

### 伪奖励

图中的立即回报（Rewards）经过求和等操作后变成累积回报，然后经TD(λ)方法去更新值函数。实际上这里的立即回报不仅仅是真实的奖励，而且包括伪奖励（Pseudo Rewards，图中没有写，但是论文里有提及）。 什么是伪奖励呢？我翻遍了全网也没见到有很多大神讲解这一块儿。按照我对原论文的理解，大概可以这么想：

![img](https://pic1.zhimg.com/v2-f11651bf6d982de14009eb4a829771e0_b.jpg)



如图所示，智能体在状态St下选择了动作a（红色的那条线），到达了后继状态St+1并获得回报Rt+1。实际上从状态St可以转到四个不同的后继状态，但是在一次试验中智能体只能转到这后继四个状态中的一个，所以只能利用一个后继状态St+1和回报Rt+1去更新当前状态St。 从利用状态和回报的角度来看，上述的指利用真实回报的更新方式似乎有些浪费，没有充分利用上下文信息。从AlphaStar的角度来看，缺少伪奖励的设置可能会使得智能体的神经网络参数偏离人类的大方向，并且过早地收敛。如果可以通过某种方法，使得智能体在一次实验中同时得到这四个后继状态的反馈，岂不是能大大增加对状态和回报的利用效率？ 这就是设置伪奖励的出发点：在更新值函数v(St)时，不仅利用真实的奖励，同时为其他几个没有经过的状态设置伪奖励，让真实奖励和伪奖励一起去更新值函数。伪奖励的设置一般会稍大于真实的奖励，从而鼓励探索，避免过早的收敛。至于伪奖励的细节，请参阅*Pseudo-reward Algorithms for Contextual Bandits with Linear Payoffff Functions*这篇论文。

### V-trace & IMPALA

智能体利用累积回报去更新策略π和值函数v的框架是IMPALA框架。什么是IMPALA呢？IMPALA是一个深度强化多任务学习架构，是A3C框架的一个改进版。IMPALA使用收集经验的体系结构，并将收集到的经验进一步传递给计算梯度的中央Learner，从而形成一个完全独立的行为和学习模型。同时，这种简单的架构，也使学习者能够加速使用显卡。如图：

![img](https://pic2.zhimg.com/v2-082ae9c22d91f258c2158c66c9ff0815_b.jpg)IMPALA图(已更正)

图中有8个Actor和2个Learner。Actors并行地根据他们自己的策略进行采样，采样得到的数据到达一定量之后就将他们传输给中心的Learner，由Learner来利用这些数据进行策略梯度的计算以及策略的更新。每隔一段时间，Actor们将Learner的策略参数复制过来变为自己的参数。 这样一来，读者可能会问了：在学习过程中Actor和Learner的策略并不完全相同，如何解决这种Off-policy的问题呢？别急，论文中的V-trace方法就是用来解决这个问题的，即减小行为动作产生的时间与Learner估计渐变时间之间的误差。V-trace的公式比较复杂，详细的分析可以参见[专栏](https://zhuanlan.zhihu.com/p/56043646)。个人觉得V-trace的主要特点是里面两个类似于重要性因子的选取，其中一个决定收敛到什么样的值函数，另外一个决定收敛的速度。另外，为了防止过早收敛，V-trace方法里在Actor的梯度公式中加了一个熵作为惩罚，即

−∇ω∑aπω(a|xs)log⁡πω(a|xs)-\nabla_{\omega} \sum_{a} \pi_{\omega}\left(a | x_{s}\right) \log \pi_{\omega}\left(a | x_{s}\right) -\nabla_{\omega} \sum_{a} \pi_{\omega}\left(a | x_{s}\right) \log \pi_{\omega}\left(a | x_{s}\right)

### UPGO

最后一个要介绍的技术是UPGO（upgoing policy upgrade）。UPGO是策略更新的一种方法，在该方法中，策略更新的方向为

ρt(GtU−Vθ(st,z))∇θlog⁡πθ(at|st,z)\rho_{t}\left(G_{t}^{U}-V_{\theta}\left(s_{t}, z\right)\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}, z\right) \rho_{t}\left(G_{t}^{U}-V_{\theta}\left(s_{t}, z\right)\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}, z\right)

其中

GtU={rt+Gt,1L if Q(st1,1,at1,z)≥Vθ(st1|1,z)rt+−Vt0(st+1,z) othervisc G_{t}^{U}=\left\{\begin{array}{ll}{r_{t}+G_{t, 1}^{L}} & {\text { if } \mathcal{Q}\left(s_{t_{1}, 1}, a_{t_{1}, z}\right) \geq V_{\theta}\left(s_{t_{1} | 1}, z\right)} \\{r_{t}^{+}-V_{t_{0}}\left(s_{t+1}, z\right)} & {\text { othervisc }}\end{array}\right.G_{t}^{U}=\left\{\begin{array}{ll}{r_{t}+G_{t, 1}^{L}} & {\text { if } \mathcal{Q}\left(s_{t_{1}, 1}, a_{t_{1}, z}\right) \geq V_{\theta}\left(s_{t_{1} | 1}, z\right)} \\{r_{t}^{+}-V_{t_{0}}\left(s_{t+1}, z\right)} & {\text { othervisc }}\end{array}\right.

ρt=min(πθ(at|st,z)πθ′(at|st,z),1) \rho_{t}=\min \left(\frac{\pi_{\theta}\left(a_{t} | s_{t}, z\right)}{\pi_{\theta^{\prime}}\left(a_{t} | s_{t}, z\right)}, 1\right) \rho_{t}=\min \left(\frac{\pi_{\theta}\left(a_{t} | s_{t}, z\right)}{\pi_{\theta^{\prime}}\left(a_{t} | s_{t}, z\right)}, 1\right)

分析这个回报G的分段函数形式，不难看出，只有当行为动作值函数优于当前的值函数时才去更新它，否则就还是使用原来的值函数，通过这样的方法来保证回报的选取是一致朝着更好的方向的，并且反向传播时不会太容易衰减掉。

### 结果分析

![img](https://pic1.zhimg.com/v2-be5c010f8bceaf4a3ca3ea1d5b279c74_b.jpg)



上图是论文中智能体Elo值（衡量智能体能力的数值，通过和历史策略打循环赛得到的分数）随学习的进行而变化的图。 从左到右，第一个红线*AlphaStar Supervised*表示只进行监督学习，没有进行强化学习时的水平；第二个红线*AlphaStar Mid*表示进行了一半强化学习时的水平（32块TPU上学习27天）；第三个红线*AlphaStar Final*表示强化学习完成后的水平（32块TPU上学习44天）。 从上到下，*Main agents*是我们所训练的主体，其分数随学习的进行而升高；*League Agents*由于每过一段时间都有一定的几率还原为人类策略模型的参数，因此策略的水平比较分散，总体上在*Main agents*和*Main exploiters*之间；*Main [exploiters](https://www.zhihu.com/search?q=exploiters&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})*由于每过一段时间也是会被还原为人类策略模型参数，因此一直保持在相对低的水平。

![img](https://pic1.zhimg.com/v2-e0ab252d7d0f6da9879a89f9a8923fc4_b.jpg)



这两张图不是很好理解，以下分析摘自[知乎专栏](https://zhuanlan.zhihu.com/p/92543229)：

>  文章还弄了一个遵循特殊策略的 held-out 策略集合，main agents 和该集合中策略比赛的结果如图 b 所示。这反映了策略学习的绝对效果。图 c 反映了不同 agent 对应的不同纳什均衡分布，这个图有点不好理解：比如 ID=40 的 agent 在大概第 25 天被造出来之后，在大概之后的五天里面都还会经常能够战胜后面新学到的策略，但是大概在 30 天之后，新产生的所有策略都完全战胜 ID=40 的策略。这说明了整个策略集合的学习过程中，新的策略能够完全战胜以前的所有策略，而没有出现前面提到的训练过程中的循环（不稳定）的情况.
>

![img](https://pic4.zhimg.com/v2-397831ef4d8c9704a566f0fe7bc65a6f_b.jpg)



上图是为了说[明学](https://www.zhihu.com/search?q=明学&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})习到的策略具有比较高的多样性。同时可以发现*Main agents*的多样性其实是少于*League exploiters*和*Main exploiters*的。

![img](https://pic4.zhimg.com/v2-ac8bd38a147dd2dec28e938d595c6c5f_b.jpg)



上图定量分析了各个技术的作用。有趣的是，在直觉中我们以为，在其他条件相同的条件下，更高的手速会给智能体带来更高的分数。但根据图g，我们发现并非如此，智能体的水平随着APM限制的放宽先升高后降低；同时，论文所设置的手速限制恰恰落在了智能体水平最高峰附近。为什么手速限制的放宽反而可能导致智能体水平降低呢？论文给出的解释是，APM限制的放宽使得智能体需要在相同时间内做出更多的决策，这使得智能体需要学习更多的“微决策”“[微操作](https://www.zhihu.com/search?q=微操作&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"102749648"})”，从而可能放慢了智能体的学习速度，拉低了（训练时间不变的条件下）智能体的水平。
