# 强化学习算法选择

- 离散动作空间推荐：**Dueling DoubleDQN（D3QN）**
- 连续动作空间推荐：擅长调参就用**TD3**，不擅长调参就用**PPO或SAC**，如果训练环境 Reward function 都是初学者写的，那就用PPO

## 离散的动作空间 discrete action space

![动图](https://pic4.zhimg.com/v2-46e75a0af992d75a844a9042ab7b5aa7_b.webp)



月球登陆器 LunarLander-v2，离散动作空间

一个离散动作空间例子：月球登陆器LunarLander-v2。它有四个引擎，每次可以开启不同方向的引擎。安全平稳降落、消耗更少燃料都会让最终得分变高。环境每一次重置 reset 都会刷新飞行器的初速度，因此需要多测几次得到的平均分才能作为给定策略的得分。

- **DQN（Deep Q Network）**可用于入门深度强化学习，使用一个Q Network来估计Q值，从而替换了 Q-table，完成从离散状态空间到连续状态空间的跨越。Q Network 会对每一个离散动作的Q值进行估计，执行的时候选择Q值最高的动作（greedy 策略）。并使用 epslion-greedy 策略进行探索（探索的时候，有很小的概率随机执行动作），来获得各种动作的训练数据。
- **DDQN（Double DQN）**更加稳定，因为最优化操作会传播高估误差，所以她同时训练两个Q network并选择较小的Q值用于计算TD-error，降低高估误差。
- **Dueling DQN**，Dueling DQN 使用了优势函数 advantage function（A3C也用了）：它只估计state的Q值，不考虑动作，好的策略能将state 导向一个更有优势的局面。原本DQN对一个state的Q值进行估计时，它需要等到**为每个离散动作收集到数据后**，才能进行准确估值。然而，在某些state下，采取不同的action并不会对Q值造成多大的影响，因此Dueling DQN 结合了 优势函数估计的Q值 与 原本DQN对不同动作估计的Q值。使得在某些state下，Dueling DQN 能在**只收集到一个离散动作的数据后**，直接得到准确的估值。当某些环境中，存在大量不受action影响的state，此时Dueling DQN能学得比DQN更快。

- **D3QN（Dueling Double DQN）。**Dueling DQN 与Double DQN 相互兼容，一起用效果很好。简单，泛用，没有使用禁忌。任何一个刚入门的人都能独立地在前两种算法的基础上改出D3QN。在论文中使用了D3QN应该引用DuelingDQN 与 DoubleDQN的文章。
- **Noisy DQN**，探索能力稍强。Noisy DQN 把噪声添加到网络的输出层之前值。原本Q值较大的动作在添加噪声后Q值变大的概率也比较大。这种探索比epslion-greedy随机选一个动作去执行更好，至少这种针对性的探索既保证了探索动作多样，也提高了探索效率。

- **Distributional RL 值分布RL（C51，Distributional Perspective RL）**。在DQN中，Q Network 拟合了Q值的期望，期望可以用一个数值去描述，比较简单。在值分布DQN中，Q Network 拟合了Q值的分布，Q值分布的描述就要麻烦一些了，但是训练效果更好。为C51的算法使用了这种方法，C表示Categorical，51表示他们将值分布划分51个grid。最终在雅达利游戏 Atari Game 上取得好结果。
- [QR-DQN](https://arxiv.org/pdf/1710.10044.pdf)**（分位数回归** [Quantile Regression](https://zhuanlan.zhihu.com/p/40681570)**）**，使用N个分位数去描述Q值分布（这种方法比C51划分51个grid的方法更妙，推荐看 [QR-DQN - Frank Tian](https://zhuanlan.zhihu.com/p/138091493)）。根据分位数的分布画出核分布曲线，详见 [Quantile-respectful density estimation based on the Harrell-Davis quantile estimator](https://aakinshin.net/posts/qrde-hd/)
- **Rainbow DQN**，上面提及的DQN变体很多是相互兼容的，因此 David Sliver 他们整合了这些变体，称为Rainbow。
- **Ape-X DQN（Distributed Prioritized Experience Replay）**，也是 David Sliver 他们做的。使用了Distributed training，用多个进程创建了多个actor去与环境交互，然后使用收集到的数据去训练同一个learner，用来加快训练速度。Prioritized Experience Replay（优先经验回放 PER 下面会讲）。Ape-X通过充分利用CPU资源，合理利用GPU，从而加快了训练速度。注意，这不等同于减少训练总步数。NVIDIA 有一个叫 Apex的库，用于加速计算。
- **Ape-X DPG（Distributed Prioritized Experience Replay）**，Ape-X算法将值分布RL应用到了 离散、连续动作空间的DQN 以及DPG 上。下面会讲连续动作空间的算法，Ape-X DPG 会和 D4PG 写在一起。
- Recurrent Experience Replay in Distributed Reinforcement Learning ICLR 2019（自称RNN版本的Ape-X，自称当时的SotA），**这是一个反面例子。请警惕使用RNN 且无开源代码的DRL论文**。由于hidden state 的存在，RNN会让MDPs退化成 PO-MDPs。



## 过渡：从离散到连续动作空间的跨越

**DQN**直接训练一个Q Network 去估计每个离散动作的Q值，使用时选择Q值大的动作去执行（贪婪策略）。**DDPG**也训练一个 Critic Network 去估计state-action的Q值，然后把Critic Network“连在”Actor Network的后面，让Critic 为**策略**网络Actor 提供优化的**梯度**。

- Value-based Methods、Policy-based Methods、Policy Gradint、Actor-Critic Methods，想弄清楚它们的区别，可以看 [Policy Gradient - Hado van Hasselt 2016 pdf](https://hadovanhasselt.files.wordpress.com/2016/01/pg1.pdf)

![img](https://pic4.zhimg.com/v2-f03bea427f233506552e3b95ed27624b_b.jpg)

Policy Gradient - Hado van Hasselt 的pdf，第6页

## 连续的动作空间 continuous action space

一个连续动作空间例子：

- LunarLanderContinuous-v2 ：它与离散动作LunarLander-v2极为相似。动作矢量是两个浮点数，符号表示喷射方向，绝对值表示喷射力度。**将离散改为连续动作后**，月球降落器的训练时间变长，但是收敛得分变高。因此在设计环境时，遵循先易后难的原则：设计初期能用离散动作尽量别用连续动作，等后期再改成连续动作，以追求更高的得分。

另一个连续动作空间例子：

- 双足机器人 BipdealWalker-v3，动作矢量控制足部关节的运动，尽快到达终点并消耗更少能量能得高分。

### 确定策略梯度

- **DPG（Deterministic Policy Gradient 确定策略梯度），**确定策略网络只输出一个确定的action，然后加上一个**人为指定的noise** 去完成探索。可以跳过DPG这篇文论，直接入门DDPG。
- **DDPG（Deep DPG ）**，可用于入门连续动作空间的DRL算法。DPG 确定策略梯度算法，直接让策略网络输出action，成功在连续动作空间任务上训练出能用的策略，但是它使用 OU-noise 这种有很多超参数的方法去探索环境，训练慢，且不稳定。
- **soft target update（软更新）**，用来稳定训练的方法，非常好用，公式是$\theta' = \tau \theta' + (1-\tau)\theta\ $，其中 theta是使用梯度进行更新的网络参数，theta' 是使用了软更新的目标网络target network参数，tau略小于1。软更新让参数的更新不至于发生剧变，从而稳定了训练。从DDPG开始就广泛使用，并且在深度学习的其他领域也能看到它的身影，如 [谷歌自监督 BYOL Bootstrap Your Own Latent](https://zhuanlan.zhihu.com/p/159765213) ，看论文的公式（1），就用了soft target update
- **TD3（TDDD，Twin Delay DDPG）**，擅长调参的人才建议用，因为它影响训练的敏感超参数很多。它从Double DQN那里继承了Twin Critic，用来降低高估误差；它用来和随机策略梯度很像的方法：计算用于更新TD-error的Q值时，给action加上了噪声，用于让Critic拟合更平滑的Q值估计函数。TD3建议 延迟更新目标网络，即多更新几次网络后，再使用 soft update 将网络更新到target network上，认为这没有多大用，后来的其他算法也不用这个技巧。TD3还建议在计算Q值时，为动作添加一个噪声，用于平滑Critic函数，在确定策略中，TD3这么用很像“随机策略”。详见 [曾伊言：强化学习算法TD3论文的翻译与解读](https://zhuanlan.zhihu.com/p/86297106)
- **D4PG（Distributed Distributional DDPG）**，这篇文章做了实验，证明了一些大家都知道好用的trick是好用的。Distributed：它像 Ape-X一样用了 多线程开了actors 加快训练速度，Distributional：Q值分布RL（看前面的C51、QR-DQN）。DDPG探索能力差的特点，它也完好无缺地继承了。

### 随机策略梯度

- **Stochastic Policy Gradient 随机策略梯度**，随机策略的探索能力更好。随机策略网络会输出action的分布（通常输出高斯分布 均值 与 方差，少数任务下用其他分布），探索的噪声大小由智能体自己决定，更加灵活。但是这对算法提出了更高的要求。
- **A3C（Asynchronous Advantage Actor-Critic）**，Asynchronous 指开启多个actor 在环境中探索，并异步更新。原本DDPG的Critic 是 Q(s, a)，根据state-action pair 估计Q值，优势函数只使用 state 去估计Q值，**这是很好的创新：降低了随机策略梯度算法估计Q值的难度**。**然而优势函数有明显缺陷**：不是任何时刻 action 都会影响 state的转移（详见 Dueling DQN），因此这个算法只适合入门学习「优势函数 advantage function」。如果你看到新论文还在使用A3C，那么你要怀疑其作者RL的水平。此外，A3C算法有离散动作版本，也有连续动作版本。A2C 指的是没有Asynchronous 的版本。
- **TRPO（Trust Region Policy Optimization）**，信任域 Trust Region。连续动作空间无法每一个动作都搜索一遍，因此大部分情况下只能靠猜。如果要猜，就只能在信任域内部去猜。TRPO将每一次对策略的更新都限制了信任域内，从而极大地增强了训练的稳定性。可惜信任域的计算量太大了，因此其作者推出了PPO，如果你PPO论文看不懂，那么建议你先看TRPO。如果你看到新论文还在使用TRPO，那么你要怀疑其作者RL的水平。
- **PPO（Proximal PO 近端策略搜索）**，训练稳定，调参简单，robust（稳健、耐操）。PPO对TRPO的信任域计算过程进行简化，论文中用的词是 surrogate objective。PPO动作的噪声方差是一个可训练的矢量（与动作矢量相同形状），而不由网络输出，这样做增强了PPO的稳健性 robustness。
- **PPO+GAE（**[Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438)**）**，训练最稳定，调参最简单，适合高维状态 High-dimensional state，**但是环境不能有太多随机因数**。GAE会根据经验轨迹 trajectory 生成优势函数估计值，然后让Critic去拟合这个值。在这样的调整下，在随机因素小的环境中，不需要太多 trajectory 即可描述当前的策略。尽管GAE可以用于多种RL算法，但是她与PPO这种On-policy 的相性最好。
- [PPG](https://arxiv.org/abs/2009.04416)**（Proximal Policy Gradient）**，A3C、PPO 都是同策略 On-policy，它要求：在环境中探索并产生训练数据的策略 与 被更新的策略网络 一定得是同一个策略。她们需要删掉已旧策略的数据，然后使用新策略在环境中重新收集。为了让PPO也能用 off-policy 的数据来训练，PPG诞生了，思路挺简单的，原本的On-policy PPO部分该干啥干啥，额外引入一个使用off-policy数据进行训练的Critic，让它与PPO的Critic共享参数，也就是Auxiliary Task，参见 [Flood Sung：深度解读：Policy Gradient，PPO及PPG](https://zhuanlan.zhihu.com/p/342150033) ，以及[白辰甲：强化学习中自适应的辅助任务加权(Adaptive Auxiliary Task Weighting)](https://zhuanlan.zhi%3Cb%3Ehu.com/p/14851%3C/b%3E6171)。这种算法并不是在任何情况下都能比PPO好，因为PPG涉及到Auxiliary task，这要求她尽可能收集更多的训练数据，并在大batch size 下面才能表现得更好。
- Soft Q-learning（Deep Energy Based Policy）是SAC的前身，最大熵算法的萌芽，她的作者后来写出了SAC（都叫soft ），可以跳过Soft QL，直接看SAC的论文。[黄伟：Soft Q-Learning论文阅读笔记](https://zhuanlan.zhihu.com/p/76681229)
- **SAC（Soft Actor-Critic with maximum entropy 最大熵）**，训练很快，探索能力好，但是很依赖Reward Function，不像PPO那样随便整一个Reward function 也能训练。PPO算法会计算新旧策略的差异（计算两个分布之间的距离），并让这个差异保持在信任域内，且不至于太小。SAC算法不是on-policy算法，不容易计算新旧策略的差异，所以它在优化时最大化策略的熵（动作的方差越大，策略的熵越高）。
- **SAC（Automating Entropy Adjustment/ Automating Temperature Parameter** $\alpha $**自动调整温度系数并维持策略的熵在某个值附近）**一般使用的SAC是这个版本的SAC，它能自动调整一个叫温度系数alpha 的超参数（温度越高，熵越大）。SAC的策略网络的优化目标=累计收益+ alpha*策略的熵。一般在训练后期，策略找到合适的action分布均值时，它的action分布方差越小，其收益越高，因而对“累计收益”进行优化，会让策略熵倾向于减小。SAC会自动选择合适的温度系数，让策略的熵保持一种适合训练的动态平衡。SAC会事先确定一个目标熵 target entropy（论文作者的推荐值是 log(action_dim)），如果策略熵大于此值，则将alpha调小，反之亦然。从这个角度看，SAC就不是最大化策略熵了，而是将策略熵限制在某个合适大小内，这点又与PPO的“保持在信任域内，且不至于太小”不谋而合。

## 混合的动作空间 hybrid action space

![img](https://pic1.zhimg.com/v2-34632b1124ddfac3e5324bbc94f57ab4_b.jpg)

王者荣耀 - 腾讯绝悟 论文截图，既有离散动作，也有连续动作


在实际任务中，混合动作的需求经常出现：如王者荣耀游戏既需要离散动作（选择技能），又需要连续动作（移动角色）。只要入门了强化学习，就很容易独立地想出以下这些方法，所以没有把它们放在前面：

- **强行使用DQN类算法，把连续动作分成多个离散动作：**不建议这么做，这破坏了连续动作的优势。一个良性的神经网络会是一个平滑的函数（k-Lipschitz 连续），相近的输入会有相似的输出。在连续的动作空间开区间[-1, +1]中，智能体会在学了-1，+1两个样本后，猜测0的样本可能介于 -1，+1 之间。而强行将拆分为离散动作 -1，0，+1之后（无论拆分多么精细），它都猜不出 0的样本，一定要收集到 0的样本才能学习。此外，精细的拆分会增加离散动作个数，尽管更加逼近连续动作，但会增加训练成本。
- [SAC for Discrete Action Space](https://arxiv.org/abs/1910.07207)**，把输出的连续动作当成是离散动作的执行概率：**SAC for Discrete Action Space 这个算法提供了将连续动作算法SAC应用在离散动作的一条技术路线：把这个输出的动作矢量当成每个动作的执行概率。一般可以直接把离散动作部分全部改成连续动作，然后套用连续动作算法，这方法简单，但是不一定最好的。
- [P-DQN](https://arxiv.org/abs/1810.06394) **（Parameterized DQN），把DQN和DDPG合起来**：Q network 会输出每个动作对应的Q值，执行的时候选择Q值高的动作。DDPG与其他策略梯度算法，让Critic预测 state-action的Q值，然后用Critic 提供的梯度去优化Actor，让Actor输出Q值高的动作。现在，对于一个混合动作来说，可以让Critic学习Q Network，让Critic也为每个离散动作输出对应的Q值，然后用Critic中 argmax Qi 提供梯度优化Actor。这是很容易独立想出来的方法，相比前两个方案缺陷更小。
- [H-PPO](https://arxiv.org/abs/1903.01344) **（Hybrid PPO），同时让策略网络输出混合动作**。连续动作（策略梯度）算法中：DDPG、TD3、SAC使用 状态-动作值函数 Q(state, action)，A3C、**PPO使用 状态值函数 Q(state)**。离散动作无法像连续动作一样将一个action输入到 Q(state, action) 里，因此 Hybird PPO选择了PPO。于是它的策略网络会像Q Network 一样为离散动作输出不同的Q值，也像PPO 一样输出连续动作。还有 [H-MPO](https://arxiv.org/abs/2001.00449)（**Hybrid MPO**），MPO是PPO算法的改进版。

## 改进经验回放，以适应稀疏奖励 sparse reward

训练LunarLander安全降落，它的奖励reward 在降落后+200，坠毁-100。当它还在空中时做任何动作都不会得到绝对值这么大的奖励。这样的奖励是稀疏的。一些算法（（其实它的奖励函数会根据飞行器在空中的稳定程度、燃料消耗给出一个很小的reward，在这种）

- **Prioritized sweeping 优先清理：**根据紧要程度调整样本的更新顺序，优先使用某些样本进行更新，用于加速训练，PER就是沿着这种思想发展出来的
- [PER](https://arxiv.org/pdf/1511.05952.pdf)**（优先经验回放 Prioritized Experience Replay）**使用不同顺序的样本进行对网络进行训练，并将不同顺序对应的Q值差异保存下来，以此为依据调整样本更新顺序，用于加速训练。
- [HER](https://papers.nips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf)**（后见经验回放 Hindsight Experience Replay）**构建可以把失败经验也利用起来的经验池，提高稀疏奖励下对各种失败探索经验的利用效率。

这种操作需要消耗CPU算力去完成。在奖励不稀疏的环境下，用了不会明显提升。在一些环境中，上面这类算法必不可少。例如 [Gym 基于收费MuJoCo 的机械臂环境 Robotics Fetch](https://gym.openai.com/envs/%23robotics) ，以及 基于[开源PyBullet的机械臂 KukaBulletEnv-v0](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/kuka.py) 。如果不用这类算法，那么需要花费更多精力去设计Reward function。

## 在RL中使用RNN

有时候，需要观察连续的几个状态才能获得完整的状态。例如赛车游戏，打砖块游戏，在只观测单独一帧图片时（部分可观测 Partially Observable state），无法知晓物体的运动速度。因此将相邻几个可观测的状态堆叠起来（stack）可以很好地应对这些难题。

然而，堆叠相邻状态无法应对所有的 PO-MDPs。于是大家想到了用RNN. 然而RNN需要使用一整段序列去训练，很不适合TD-errors（贝尔曼公式）的更新方式。如图：普通Actor网络的输入只有state，而使用RNN的Actor网络的输入其实是 Partially Observable state 以及 hidden state。这意味着输入critic网络进行Q值评估的state不完整（不包含RNN内部的hidden state）。为了解决**环境的PO-MDPs难题**，直接引入RNN结构。然而引入RNN结构的行为又给RL带来了**RNN内部的PO-MDPs难题**。

![img](https://pic4.zhimg.com/v2-bfb5ca44f103cf1de24aaff13c74c527_b.jpg)

尽管在on-policy算法中，如果使用完整的轨迹（trajectory）进行更新，那么可以缓解critic观测不到 hidden state给训练到来的影响。（如腾讯绝悟的PPO算法使用了RNN），但是这个问题还是没有很好地得到解决。

## 强化学习探索 Exploration Strategies

收集不同的state、在同一个state下尝试不同的action 的探索过程非常重要。通过探索收集到足够多的数据，是用RL训练出接近最优策略的前提。下面这篇系统介绍了各种探索策略：

[Exploration Strategies in Deep Reinforcement Learning - LiLianWeng](https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html) （也推荐看她的其他文章）

> 最近的 First return, then explore Nature.2021  是Go-explore. 2018的升级版

## 多智能体算法 MultiAgent RL

多智能体算法的综述：[An Overview of Multi-agent Reinforcement Learning from Game Theoretical Perspective](https://arxiv.org/abs/2011.00583) - 2020-12。在比对了多份MARL综述后，只推荐这一篇，它的目录与的注释如下：

```python3
Contents
1 Introduction
    ...
    1.3 2019: A Booming Year for MARL  # 写在2019年前的MARL综述可看性低
2 Single-Agent Reinforcement Learning
    ...
3 Multi-Agent Reinforcement Learning
    ...
    3.2.5 Partially Observable Settings  # state部分可观测的MDPs
    3.3 Problem Formulation: Extensive-Form Game
    3.3.1 Normal-Form Representation  # 普通形式
    3.3.2 Sequence-Form Representation  # 序列形式
    3.4 Solving Extensive-form Games
    3.4.1 Solutions to Perfect-Information Games  # 完全信息博弈
    3.4.2 Solutions to Imperfect-Information Games  # 非完全信息博弈（有战场迷雾）
4 The Grand Challenges 38
    4.1 The Combinatorial Complexity  # 动作空间变大，搜索策略变难
    4.2 The Multi-Dimensional Learning Objectives  # 状态空间变大，搜索策略变难
    4.3 The Non-Stationarity Issue  # MARL中，每个智能体的策略总发生改变，导致外部环境不稳定
    4.4 The Scalability Issue when N >> 2  # 智能体数量增大，甚至数量改变
5 A Survey of MARL Surveys  # 推荐有单智能体基础的人得先看 MARL算法的分类
    ...
6 Learning in Identical-Interest Games  # 合作的MARL
    6.1 Stochastic Team Games
    6.1.1 Solutions via Q-function Factorisation  # 基于Q值
    6.1.2 Solutions via Multi-Agent Soft Learning   # 基于随机策略
    6.2 Dec-POMDP  #  decentralized PO-MDPs 每个智能体智能看到局部的state 导致的部分可观测
    6.3 Networked Multi-Agent MDP  智能体是异构的heterogeneous，而非同源homogeneous
    6.4 Stochastic Potential Games
7 Learning in Zero-Sum Games  # 竞争的MARL （包含了团体间竞争，团体内合作的情况）
    ...
    7.3.1 Variations of Fictitious Play  # 虚拟博弈， 类似于 Model-based 做 Planning
    7.3.2 Counterfactual Regret Minimisation  # 反事实推理（向左错了，于是考虑向右是否更好）
    7.4 Policy Space Response Oracle  # 策略空间太大时，考虑用元博弈(meta-game)
    7.5 Online Markov Decision Process
    7.6 Turn-Based Stochastic Games  # 智能体之间轮流做决策，而不是同时做
8 Learning in General-Sum Games
    ...  # 混合了团队博弈合作team games 与 零和博弈竞争zero-sum games 的 General-Sum game
9 Learning in Games with N → +∞  # 无终止状态的博弈（需要考虑信任与背叛）
    9.1 Non-Cooperative Setting: Mean-Field Games  # 博弈平均场，把其他智能体也视为外部环境
    ...
END
```

部分多智能体算法的代码以及少量介绍：[starry-sky6688 在星际争霸环境 复现了多种多智能体强化学习算法](https://github.com/starry-sky6688/StarCraft)

若智能体间通信没有受到限制（不限量，无延迟），那么完全可以把多智能体当成单智能体来处理。适用于部分可观测的MDPs的算法（[Partially observable MDPs](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process)），在多智能体任务中，每个视角有限的智能体观察到的只是 partially observable state。很多多智能体算法会参与 PO-MDPs 的讨论，由于每个智能体只能观察到局部信息而导致的部分可观测被称为 Dec-POMDP，在上面的MARL综述也有讨论。

多智能体强化学习算法在2021年前，**只有QMix（基于Q值分解+DQN）和MAPPO（基于MADDPG提出的CTDE框架+PPO）这两个算法可信**.

## 分层强化学习 Hierarchical RL

神经网络有一个缺陷（特性）：在数据集A上面训练的网络，拿到数据集B上训练后，这个网络会把数据集A学到的东西忘掉（灾难性遗忘 [Catastrophic forgetting 1999](https://www.sciencedirect.com/science/article/pii/S1364661399012942)）。如果让智能体学游泳，再让它学跑步，它容易把游泳给忘了（人好像也这样，不够没有它那么严重）。深度学习领域有「迁移学习」、强化学习领域有「分层强化学习」在试图解决这些难题。

- **FuNs，分级网络 FeUdal Networks** ，分层强化学习不再用单一策略去解决这些更复杂的问题，而是将策略分为上层策略与多个下层策略 sub-policy 。上层策略会根据不同的状态决定使用哪个下层策略。它使用了同策路on-policy的A3C算法
- **HIRO，使用异策略进行校正的分层强化学习 HIerarchical Reinforcement learning with Off-policy correction**，警惕HIRO这个算法：FuN使用同策路on-policy的A3C算法，HIRO使用异策略off-policy的TD3算法，这个让警惕：个人认为不能像HIRO那样去使用TD3算法。
- **Option-Critic，有控制权的下层策略**，让将上层的策略和下层策略的控制权也当成是可以学习的，让下层的策略学习把“决定使用哪个策略的选择权”交还给上层策略的时机，这是一种隐式的分层强化学习方案，没有复现过这个算法，不确定这是否真的有效。

## 逆向强化学习 Inverse RL 与 模仿学习 Imitation Learning

强化学习会在回报函数 Reward function的指导下探索训练环境，并使用未来的期望收益来强化当前动作，试图求出更优的策略。然而，现实中不容易找到需要既懂任务又懂RL的人类去手动设计Reward function。

> 以 LunarLander为例子：降落+200 坠毁-100，消耗燃料会扣0~100。其实只有这些也能用很长的时间训练得到能安全降落的飞行器。但实际上，还可以根据飞行器的平稳程度给它每步一位数的奖惩，根据飞行器距离降落点的距离给他额外的奖励。这些很细节的调整可以减少智能体的训练时间。所以前面建议：如果训练环境 Reward function 都是初学者写的，那就用PPO。等到 Reward function 设计得更合理之后，才适合用SAC。

- 强化学习：训练环境+DRL算法+Reward Function = 搜索出好的策略
- 逆向强化学习：训练环境+IRL算法+好的策略 = 逆向得到Reward Function

逆向强化学习为了解决这个问题，提出：通过模仿好的策略 去反向得到 Reward function。综述：[A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress](https://arxiv.org/abs/1806.06877)

## 基于模型的强化学习算法 Model-based RL（重点介绍MuZero）

这里的「模型」指：状态转移模型。离散状态空间下的状态转移模型可以用 状态转移矩阵去描述。基于模型的算法需要将状态转移模型探索出来（或由人类提供），而 无模型算法 model-free RL 不需要探索出模型，它仅依靠智能体在环境中探索 rollout 得到的一条条 trajectory 中记录的 environment transition (s, a, r, next state) 即可对策略进行更新。

综述：[Model-based Reinforcement Learning: A Survey](https://arxiv.org/abs/2006.16712) 。OpenAI 提供了一些简单的代码： [SpinningUp Model-based RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html) 。

**近年来受到最多圈外人关注的 model-based RL 是 MuZero**。在下棋、雅达利游戏这种状态转移模型相对容易拟合的离散动作空间任务中，MuZero取得了非常不错的表现。它有三个网络：

- **编码器：**输入连续观测到的几个state，将其编码成 latent state。为何非要使用 latent state 而不直接使用 state？ 在当前state 下做出action 后，并不会转移到某个确切的状态，next state 是一个不容易直接描述的分布。因此接下来的生成器不会（也无法）直接预测 next state，只能预测 latent state。
- **预测器：**输入当前观测到的state，生成执行每个动作的概率，并预测执行每个动作的value （Q值，不反对将它粗略地理解为DQN的 Q Network）。
- **生成器：**输入当前观测到的state，生成 执行每个离散动作后会转移到的 latent state 以及对应的 Reward（这是单步的Reward，不是累加得到的Q值）。生成器就是MuZero 这个model-based RL算法 学到的状态转移模型。

如果离散动作的数量很多（如围棋），那么MuZero 会使用MCTS（[Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) 蒙特卡洛树搜索），剪除低概率的分支并估计Q值（论文里用  ），具体剪去多少分支要看有多少算力和时间。

![img](https://pic2.zhimg.com/80/v2-f11ec27c059ff5fa607351655ecfe5dd_720w.jpg)

图中左上角绿色的柱子是离散动作执行概率，可以看到：虽然围棋有很多位置可下，但实际上只有几个位置能下出好棋。如果对面不是柯洁，那么一些分支可以剪除不去计算它。

> 剪枝：蒸馏学习的 Weight Pruning 权重剪枝，消除权重中不必要的值。在MCTS中的剪枝是不计算执行概率过低的动作分支。

model-based RL 学到状态转移模型之后，就能在探索环境之前想象出接下来几步的变化，然后基于环境模型做规划，减少与环境的交互次数。（在model-based RL 中经常可以读到  Imagination，planning，dream这些词）。

## 好用的强化学习算法是？

- 没有很多需要调整的超参数。D3QN、SAC超参数较少，且SAC可自行调整超参数
- 超参数很容易调整或确定。SAC的 reward scaling 可以在训练前直接推算出来。PPO超参数的细微改变不会极大地影响训练
- 训练快，收敛稳、得分高。看下面的学习曲线 learning curve

![img](https://pic4.zhimg.com/80/v2-9d8ff9652e255dbf6d41e754a9b0d997_720w.jpg)

## 学习曲线怎么看？

- 横轴可以是训练所需的步数（智能体与环境交互的次数）、训练轮数（达到固定步数、失败、通关 就终止终止这一轮的训练episode）、训练耗时（这个指标还与设备性能有关）
- 纵轴可以是 每轮得分（ 每一轮的每一步的reward 加起来，episode return），对于没有终止状态的任务，可以计算某个时间窗口内reward之和
- 有时候还有用 plt.fill_between 之类的上下std画出来的波动范围，用于让崎岖的曲线更好看一点：先选择某一段数据，然后计算它的均值，再把它的标准差画出来，甚至可以画出它的上下偏差（琴形图）。如果同一个策略在环境随机重置后得分相差很大，那么就需要多测几次。

## 好的算法的学习曲线应该是？

- 训练快，曲线越快达到某个目标分数 target reward （需要多测几次的结果才有说服力）
- 收敛稳，曲线后期不抖动（曲线在前期剧烈抖动是可以接受的）
- 得分高，曲线的最高点可以达到很高（即便曲线后期下降地很厉害也没关系，因为可以保存整个训练期间“平均得分”最高的模型）。

![img](https://pic3.zhimg.com/v2-014c0ec7c555455856c4380a5a4541fe_b.jpg)![img](https://pic3.zhimg.com/80/v2-014c0ec7c555455856c4380a5a4541fe_720w.jpg)

截图来自 Parametrized DQN 的图5，这是正常的learning curve
